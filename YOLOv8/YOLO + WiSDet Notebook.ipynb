{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FFRiXAR_pf"
      },
      "source": [
        "### SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BzcQ27Fn09GC"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseDetector(ABC):\n",
        "    \"\"\"Abstract base class for object detection models in the Guided Pipeline.\n",
        "\n",
        "    The Guided Pipeline usually has two stages:\n",
        "      - A guide or windshield detector that finds the region of interest.\n",
        "      - A sticker detector that runs inside those regions.\n",
        "\n",
        "    Subclasses can choose their own argument signatures for training and\n",
        "    prediction (for example using a YOLO data YAML path), but they must\n",
        "    follow the return formats described in the method docstrings so that\n",
        "    the pipeline can consume their outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_model(self, data, **kwargs):\n",
        "        \"\"\"Train the detector model.\n",
        "\n",
        "        Typical usage in the Guided Pipeline is to train the guide or\n",
        "        windshield detector, but this method can also be used to train\n",
        "        a sticker detector if needed.\n",
        "\n",
        "        Args:\n",
        "            data: Training data used to fit the model. The concrete\n",
        "                implementation defines the exact type, for example:\n",
        "                - Path to a YOLO data YAML file.\n",
        "                - A dataset object or data loader.\n",
        "                - A list of image paths or a dataset root directory.\n",
        "            **kwargs: Additional keyword arguments for training. Implementations\n",
        "                can support items such as:\n",
        "                - Hyperparameters (epochs, batch size, learning rate).\n",
        "                - Device selection.\n",
        "                - Logging and checkpoint options.\n",
        "\n",
        "        Returns:\n",
        "            None. Implementations may optionally return internal training\n",
        "            results, but the Guided Pipeline does not rely on a return value.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict_windshield(self, data, **kwargs):\n",
        "        \"\"\"Run guide or windshield detection on full images.\n",
        "\n",
        "        This method must run inference on full images and return the best\n",
        "        guide or windshield bounding box per image in a standard format.\n",
        "\n",
        "        Args:\n",
        "            data: Input data to run inference on. The concrete implementation\n",
        "                defines the exact type, for example:\n",
        "                - Path to a YOLO data YAML file, which is then used to find\n",
        "                  an image directory for a split.\n",
        "                - A list of image paths.\n",
        "                - A dataset object or iterable of images.\n",
        "            **kwargs: Extra keyword arguments to control inference such as\n",
        "                confidence threshold, IoU threshold, image size, device,\n",
        "                batch size, or whether to save visualizations.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: A list of dictionaries, one per image, with the\n",
        "            following structure:\n",
        "\n",
        "                [\n",
        "                    {\n",
        "                        \"image_path\": \"<path/to/image.jpg>\",\n",
        "                        \"guides\": [\n",
        "                            {\n",
        "                                \"xyxy\": [x1, y1, x2, y2],\n",
        "                                \"conf\": float_confidence,\n",
        "                                \"cls\": class_id\n",
        "                            }\n",
        "                        ]\n",
        "                    },\n",
        "                    ...\n",
        "                ]\n",
        "\n",
        "            Notes:\n",
        "                - \"image_path\" must be the path of the corresponding image.\n",
        "                - \"guides\" is a list of predicted guide boxes. Implementations\n",
        "                  may choose to keep only the best box (like YOLODetector) or\n",
        "                  multiple boxes, but the value must always be a list.\n",
        "                - If an image has no detections, \"guides\" must be an empty list.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict_sticker(self, data, **kwargs):\n",
        "        \"\"\"Run sticker detection on cropped or full images.\n",
        "\n",
        "        This method is intended for the second stage of the Guided Pipeline.\n",
        "        It typically receives cropped windshield images or a split of a\n",
        "        dataset and returns all sticker detections for each image.\n",
        "\n",
        "        Args:\n",
        "            data: Input data for sticker inference. The concrete implementation\n",
        "                defines the exact structure, for example:\n",
        "                - Path to a YOLO data YAML file for a sticker crop dataset.\n",
        "                - A list of crop image paths.\n",
        "                - An iterable of images or crop records.\n",
        "            **kwargs: Extra keyword arguments to control inference such as\n",
        "                confidence threshold, IoU threshold, image size, device,\n",
        "                batch size, or output saving options.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: A list of dictionaries, one per image or crop, with\n",
        "            the following structure (as used by YOLODetector):\n",
        "\n",
        "                [\n",
        "                    {\n",
        "                        \"crop_path\": \"<path/to/crop_or_image.jpg>\",\n",
        "                        \"boxes\": [\n",
        "                            {\n",
        "                                \"xyxy\": [x1, y1, x2, y2],\n",
        "                                \"conf\": float_confidence,\n",
        "                                \"cls\": class_id\n",
        "                            },\n",
        "                            ...\n",
        "                        ]\n",
        "                    },\n",
        "                    ...\n",
        "                ]\n",
        "\n",
        "            Notes:\n",
        "                - \"crop_path\" should point to the image or crop used for\n",
        "                  sticker prediction.\n",
        "                - \"boxes\" is a list of all predicted sticker bounding boxes.\n",
        "                - If an image has no detections, \"boxes\" must be an empty list.\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecDHWxBgxa0n"
      },
      "source": [
        "### YOLOv8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qChKS9hk-mV",
        "outputId": "97c0a0b9-a784-4959-baa0-ed6d0492f50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import yaml, os, cv2, uuid, shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from shutil import copy2\n",
        "\n",
        "# from base_detector import BaseDetector\n",
        "\n",
        "\n",
        "class YOLODetector(BaseDetector):\n",
        "    \"\"\"YOLOv8 based implementation of :class:`BaseDetector` for the Guided Pipeline.\n",
        "\n",
        "    This class wraps an ``ultralytics.YOLO`` model and provides:\n",
        "\n",
        "      * Training utilities that integrate with a Reduce-on-Plateau scheduler.\n",
        "      * Windshield prediction on full images in the Guided Pipeline format.\n",
        "      * Sticker prediction on cropped images.\n",
        "\n",
        "    The class is intended to be used as the guide or sticker detector within a\n",
        "    two stage Guided Pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained: str = \"yolov8n.pt\",\n",
        "        lr0: float = 0.001,\n",
        "        momentum: float = 0.95,\n",
        "        weight_decay: float = 0.0005,\n",
        "        freeze: int = 10,\n",
        "        ReduceLRonPlateau: bool = False,\n",
        "    ):\n",
        "        \"\"\"Initialize a YOLODetector instance.\n",
        "\n",
        "        Args:\n",
        "            pretrained: Path to a pretrained YOLOv8 checkpoint or a model name\n",
        "                that ``ultralytics.YOLO`` can load, for example ``\"yolov8n.pt\"``.\n",
        "            lr0: Initial learning rate used during training.\n",
        "            momentum: Momentum parameter for the SGD optimizer.\n",
        "            weight_decay: Weight decay (L2 regularization) used during training.\n",
        "            freeze: Number of layers or blocks that YOLO should freeze during\n",
        "                training. Passed to ``YOLO.train`` via the ``freeze`` argument.\n",
        "            ReduceLRonPlateau: If ``True``, enable learning rate scheduling using\n",
        "                the custom ``ReduceOnPlateauMAP50_WithDetectorNNClone`` callback.\n",
        "        \"\"\"\n",
        "        self.pretrained = pretrained\n",
        "        self.model = YOLO(pretrained)\n",
        "\n",
        "        self.lr0 = lr0\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.freeze = freeze\n",
        "        self.ReduceLRonPlateau = ReduceLRonPlateau\n",
        "\n",
        "        print('pretrained:', pretrained)\n",
        "\n",
        "    def get_split_dir(self, yaml_path, split: str = \"train\") -> str:\n",
        "        \"\"\"Get the image directory for a given split from a YOLO data YAML.\n",
        "\n",
        "        This helper assumes the standard directory layout:\n",
        "\n",
        "            <dataset_root>/\n",
        "                data.yaml\n",
        "                train/\n",
        "                    images/\n",
        "                    labels/\n",
        "                val/\n",
        "                    images/\n",
        "                    labels/\n",
        "                test/\n",
        "                    images/\n",
        "                    labels/\n",
        "\n",
        "        Args:\n",
        "            yaml_path: Path to a YOLO data YAML file.\n",
        "            split: Dataset split key such as ``\"train\"``, ``\"val\"`` or ``\"test\"``.\n",
        "\n",
        "        Returns:\n",
        "            The path to the corresponding ``<split>/images`` directory as a\n",
        "            string.\n",
        "        \"\"\"\n",
        "        return str(Path(yaml_path).parent / split / \"images\")\n",
        "\n",
        "    def _ensure_overrides_model(self):\n",
        "        \"\"\"Ensure that the underlying YOLO model has a valid ``overrides['model']``.\n",
        "\n",
        "        Some ultralytics workflows expect ``model.overrides['model']`` to contain\n",
        "        the model configuration path or checkpoint. This method sets that field\n",
        "        if it is missing or ``None``, using either ``model.cfg`` or the\n",
        "        ``pretrained`` path passed at initialization.\n",
        "        \"\"\"\n",
        "        m = self.model\n",
        "        if not hasattr(m, \"overrides\"):\n",
        "            m.overrides = {}\n",
        "        if \"model\" not in m.overrides or m.overrides[\"model\"] is None:\n",
        "            m.overrides[\"model\"] = getattr(m, \"cfg\", None) or self.pretrained\n",
        "\n",
        "    ########################################################\n",
        "    def train_model(\n",
        "        self,\n",
        "        data_yaml,\n",
        "        classes=None,\n",
        "        epochs: int = 300,\n",
        "        imgsz: int = 1200,\n",
        "        seed: int = 30,\n",
        "        AP_split: str = 'train',\n",
        "        yolo_names=['car-sticker'],\n",
        "        scheduled_epochs=None,\n",
        "        conf: float = 0.5,\n",
        "        iou: float = 0.5,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Train the YOLO model as a guide or windshield detector.\n",
        "\n",
        "        This method wraps ``ultralytics.YOLO.train`` and optionally attaches a\n",
        "        ``ReduceOnPlateauMAP50_WithDetectorNNClone`` callback to adjust the\n",
        "        learning rate based on AP@50 on a COCO style evaluation.\n",
        "\n",
        "        The intent is to train the first stage of the Guided Pipeline on\n",
        "        windshield or guide bounding boxes.\n",
        "\n",
        "        Args:\n",
        "            data_yaml: Path to the YOLO data YAML that defines train and val\n",
        "                splits and class names.\n",
        "            classes: Optional list of class indices to train on. If ``None``,\n",
        "                YOLO uses all classes defined in the YAML.\n",
        "            epochs: Number of training epochs.\n",
        "            imgsz: Input image size. Passed to YOLO as ``imgsz``.\n",
        "            seed: Random seed for reproducibility.\n",
        "            AP_split: Dataset split to use when evaluating AP within the\n",
        "                scheduler, for example ``\"train\"`` or ``\"val\"``.\n",
        "            yolo_names: List of class names used when computing AP metrics in\n",
        "                the scheduler.\n",
        "            scheduled_epochs: Optional list of epochs where the LR should be\n",
        "                changed explicitly by the scheduler. If ``None``, no fixed\n",
        "                schedule is applied.\n",
        "            conf: Confidence threshold used during internal AP evaluation.\n",
        "            iou: IoU threshold used during internal AP evaluation.\n",
        "            **kwargs: Additional keyword arguments forwarded to\n",
        "                ``YOLO.train``, such as augmentation options or logging flags.\n",
        "\n",
        "        Returns:\n",
        "            None. The underlying YOLO training call returns a ``Results``\n",
        "            object, but this method does not forward it. Training history and\n",
        "            test metrics are recorded inside the scheduler callback.\n",
        "        \"\"\"\n",
        "        if scheduled_epochs is None:\n",
        "            scheduled_epochs = []\n",
        "\n",
        "        Global_History = []\n",
        "        Test_History = []\n",
        "\n",
        "        hyp_arr = {\n",
        "            'lr0': self.lr0,\n",
        "            'momentum': self.momentum,\n",
        "            'weight_decay': self.weight_decay,\n",
        "        }\n",
        "\n",
        "        hyp = hyp_arr\n",
        "        detector_factory = lambda: YOLODetector(pretrained=self.pretrained)     # Model copy\n",
        "\n",
        "        plateau_cb = ReduceOnPlateauMAP50_WithDetectorNNClone(\n",
        "            # ReduceOnPlateaurAP50\n",
        "            use_plateau=self.ReduceLRonPlateau,                                  # turn off patience based logic\n",
        "            factor=0.1,\n",
        "            patience=10,                                                         # first plateau needs 100 bad epochs\n",
        "            patience_after_first=10,                                             # after first LR drop, use 50\n",
        "            cooldown=0,\n",
        "            min_lr=0.000001, warmup_epochs=0, start_after_map=-1.0,             # patience counts only after AP50 > ...\n",
        "\n",
        "            # MultiStepLR\n",
        "            scheduled_epochs=scheduled_epochs,                                   # reduce exactly at these epochs\n",
        "            scheduled_factors={},                                                # fallback to .factor if missing Epoch:Factor\n",
        "\n",
        "            # For AP Generation\n",
        "            owner=self,\n",
        "            detector_factory=detector_factory,\n",
        "            evaluate_fn=evaluate_fn,\n",
        "            coco_split_dir=COCO_SPLIT,\n",
        "            yolo_names=yolo_names,\n",
        "            predict_kwargs=dict(\n",
        "                data_yaml=DATA_YAML, classes=classes,\n",
        "                imgsz=(800, 1200), AP_split=AP_split,\n",
        "                conf=conf, iou=iou, verbose=False, save=False\n",
        "            ),\n",
        "            eval_every=10,\n",
        "            clone_device=\"cuda:0\",                                               # \"cpu\" or \"cuda:0\" if you prefer speed\n",
        "            verbose=True,\n",
        "\n",
        "            # Only for epoch testing, this does not affect training\n",
        "            test_eval_start_epoch=999999999,\n",
        "            test_eval_epochs=[],\n",
        "            test_eval_every=99999,\n",
        "            test_predict_kwargs=dict(                                            # only deltas from predict_kwargs are needed\n",
        "                split=\"test\",                                                    # ensure test split\n",
        "                data_yaml=DATA_YAML\n",
        "            ),\n",
        "            test_coco_split_dir=COCO_SPLIT_TEST\n",
        "        )\n",
        "\n",
        "        # Register hooks\n",
        "        self.model.add_callback(\"on_train_epoch_start\", plateau_cb.on_train_epoch_start)\n",
        "        self.model.add_callback(\"on_fit_epoch_end\", plateau_cb)\n",
        "        self.model.add_callback(\"on_fit_end\", plateau_cb.on_fit_end)\n",
        "        self.model.add_callback(\"on_train_end\", plateau_cb.on_train_end)\n",
        "\n",
        "        # Train\n",
        "        results = self.model.train(\n",
        "            data=data_yaml, classes=classes,\n",
        "            epochs=epochs, imgsz=imgsz,\n",
        "            conf=conf, iou=iou,\n",
        "            optimizer=\"SGD\", lr0=hyp['lr0'],\n",
        "            momentum=hyp['momentum'],\n",
        "            weight_decay=hyp['weight_decay'],\n",
        "            seed=seed,\n",
        "            batch=4,\n",
        "            device=0,\n",
        "            cos_lr=False,\n",
        "            lrf=1.0,\n",
        "            warmup_epochs=0,\n",
        "            freeze=self.freeze,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        Global_History.append({'history': plateau_cb.history})\n",
        "        Test_History.append({'Metrics:': plateau_cb.test_ap_history})\n",
        "\n",
        "    ########################################################\n",
        "    def predict_windshield(\n",
        "        self,\n",
        "        data_yaml,\n",
        "        classes=None,\n",
        "        imgsz=(800, 1200),\n",
        "        split: str = 'test',\n",
        "        conf: float = 0.5,\n",
        "        iou: float = 0.5,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Run guide or windshield detection on full images.\n",
        "\n",
        "        This method scans the image directory corresponding to the requested\n",
        "        split in the given YOLO data YAML, runs YOLO inference, and returns the\n",
        "        best windshield bounding box per image in the Guided Pipeline format.\n",
        "\n",
        "        Args:\n",
        "            data_yaml: Path to the YOLO data YAML file.\n",
        "            classes: Optional list of class indices to detect. If ``None``, use\n",
        "                all classes.\n",
        "            imgsz: Image size (height, width) to use during inference.\n",
        "            split: Dataset split key, for example ``\"train\"`` or ``\"test\"``.\n",
        "            conf: Minimum confidence threshold for YOLO predictions.\n",
        "            iou: IoU threshold used for internal non maximum suppression.\n",
        "            **kwargs: Additional keyword arguments passed to\n",
        "                ``YOLO.predict`` such as device, half precision, or visualization\n",
        "                flags.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: A list with one entry per image, where each entry has\n",
        "            the structure:\n",
        "\n",
        "                {\n",
        "                    \"image_path\": \"<absolute or relative image path>\",\n",
        "                    \"guides\": [\n",
        "                        {\n",
        "                            \"xyxy\": [x1, y1, x2, y2],\n",
        "                            \"conf\": float_confidence,\n",
        "                            \"cls\": class_id\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "            If an image has no detections, the ``\"guides\"`` list is empty.\n",
        "        \"\"\"\n",
        "        data_dir = self.get_split_dir(data_yaml, split)\n",
        "        print('data_dir: ', data_dir)\n",
        "\n",
        "        stream = self.model.predict(\n",
        "            source=data_dir,\n",
        "            conf=conf, iou=iou,\n",
        "            classes=classes,\n",
        "            imgsz=imgsz,\n",
        "            stream=True,\n",
        "            save=True,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        predicted_ws = []\n",
        "\n",
        "        for r in stream:\n",
        "            img_path = str(r.path)\n",
        "\n",
        "            if r.boxes is None or len(r.boxes) == 0:\n",
        "                predicted_ws.append({\"image_path\": img_path, \"guides\": []})\n",
        "                continue\n",
        "\n",
        "            # tensors -> numpy\n",
        "            xyxy = r.boxes.xyxy.cpu().numpy()              # (N,4) [x1,y1,x2,y2]\n",
        "            score = r.boxes.conf.cpu().numpy()             # (N,)\n",
        "            cls = r.boxes.cls.cpu().numpy() if r.boxes.cls is not None else None\n",
        "\n",
        "            # sort by confidence (desc)\n",
        "            order = score.argsort()[::-1]\n",
        "            xyxy = xyxy[order]\n",
        "            score = score[order]\n",
        "            if cls is not None:\n",
        "                cls = cls[order]\n",
        "\n",
        "            # index of best detection\n",
        "            best_idx = int(score.argmax())\n",
        "\n",
        "            b = xyxy[best_idx].tolist()\n",
        "            g = {\"xyxy\": b, \"conf\": float(score[best_idx])}\n",
        "            if cls is not None:\n",
        "                g[\"cls\"] = int(cls[best_idx])\n",
        "\n",
        "            print(f\"{img_path}: keeping best box -> conf={score[best_idx]:.3f}\")\n",
        "\n",
        "            predicted_ws.append({\"image_path\": img_path, \"guides\": [g]})\n",
        "\n",
        "        return predicted_ws\n",
        "\n",
        "    def predict_sticker(\n",
        "        self,\n",
        "        data_yaml,\n",
        "        classes=None,\n",
        "        imgsz=(800, 1200),\n",
        "        split: str = 'train',\n",
        "        conf: float = 0.5,\n",
        "        iou: float = 0.5,\n",
        "        save: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Run sticker detection on cropped or full images.\n",
        "\n",
        "        This method operates on all images in the specified split of the data\n",
        "        YAML, runs YOLO inference, and returns all bounding box predictions for\n",
        "        each image.\n",
        "\n",
        "        It is intended for the second stage of the Guided Pipeline where the\n",
        "        model predicts stickers inside previously cropped windshield regions or\n",
        "        inside full images.\n",
        "\n",
        "        Args:\n",
        "            data_yaml: Path to the YOLO data YAML file.\n",
        "            classes: Optional list of class indices to detect. If ``None``, use\n",
        "                all classes.\n",
        "            imgsz: Image size (height, width) to use during inference.\n",
        "            split: Dataset split key, for example ``\"train\"`` or ``\"test\"``.\n",
        "            conf: Minimum confidence threshold for YOLO predictions.\n",
        "            iou: IoU threshold used for internal non maximum suppression.\n",
        "            save: If ``True``, YOLO saves visualization outputs in its standard\n",
        "                ``runs/detect`` directory.\n",
        "            **kwargs: Additional keyword arguments forwarded to\n",
        "                ``YOLO.predict``.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: A list with one entry per image, where each entry has\n",
        "            the structure:\n",
        "\n",
        "                {\n",
        "                    \"crop_path\": \"<image path>\",\n",
        "                    \"boxes\": [\n",
        "                        {\n",
        "                            \"xyxy\": [x1, y1, x2, y2],\n",
        "                            \"conf\": float_confidence,\n",
        "                            \"cls\": class_id\n",
        "                        },\n",
        "                        ...\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "            If an image has no detections, ``\"boxes\"`` is an empty list.\n",
        "        \"\"\"\n",
        "        data_dir = self.get_split_dir(data_yaml, split)\n",
        "        print('data_dir: ', data_dir)\n",
        "\n",
        "        print(\"Prediction Start...\\n\\n\")\n",
        "\n",
        "        results = self.model.predict(\n",
        "            source=data_dir,\n",
        "            conf=conf, iou=iou,\n",
        "            imgsz=imgsz,\n",
        "            classes=classes,\n",
        "            stream=True,\n",
        "            save=save\n",
        "        )\n",
        "\n",
        "        preds = []\n",
        "        for i, r in enumerate(results, 1):\n",
        "            path = str(r.path)\n",
        "            H, W = r.orig_shape  # (H, W)\n",
        "            ms = float(r.speed['inference'])\n",
        "\n",
        "            if r.boxes is None or len(r.boxes) == 0:\n",
        "                preds.append({\"crop_path\": path, \"boxes\": []})\n",
        "                continue\n",
        "\n",
        "            xyxy = r.boxes.xyxy.cpu().numpy()\n",
        "            confs = r.boxes.conf.cpu().numpy()\n",
        "            clses = r.boxes.cls.cpu().numpy() if r.boxes.cls is not None else None\n",
        "\n",
        "            # sort by confidence (desc), keep all\n",
        "            order = confs.argsort()[::-1]\n",
        "            xyxy = xyxy[order]\n",
        "            confs = confs[order]\n",
        "            if clses is not None:\n",
        "                clses = clses[order]\n",
        "\n",
        "            boxes = []\n",
        "            for j in range(len(confs)):\n",
        "                b = {\n",
        "                    \"xyxy\": xyxy[j].tolist(),\n",
        "                    \"conf\": float(confs[j])\n",
        "                }\n",
        "                if clses is not None:\n",
        "                    b[\"cls\"] = int(clses[j])\n",
        "                boxes.append(b)\n",
        "\n",
        "            preds.append({\"crop_path\": path, \"boxes\": boxes})\n",
        "\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYtbPHrFSDkK"
      },
      "source": [
        "### PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fTvIKgWvbW6p"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import glob\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json, time\n",
        "\n",
        "\n",
        "class GuidedPipeline:\n",
        "    \"\"\"Two stage guided detection pipeline for windshield and sticker detection.\n",
        "\n",
        "    This pipeline coordinates two detectors that implement :class:`BaseDetector`:\n",
        "\n",
        "      * ``guide`` - a guide or windshield detector that runs on full images and\n",
        "        returns guide bounding boxes in the standard Guided Pipeline format:\n",
        "        ``{\"image_path\": str, \"guides\": [{\"xyxy\": [...], \"conf\": float, \"cls\": int}, ...]}``.\n",
        "      * ``detector`` - a sticker detector that runs on cropped windshield\n",
        "        regions and returns sticker predictions in the standard format used by\n",
        "        :meth:`BaseDetector.predict_sticker`.\n",
        "\n",
        "    The typical flow is:\n",
        "\n",
        "      1. Train the guide detector on full images for the windshield class.\n",
        "      2. Use the guide detector to predict windshields and crop those regions.\n",
        "      3. Build a YOLO dataset of sticker crops.\n",
        "      4. Train the sticker detector on that crop dataset.\n",
        "      5. At inference time, repeat steps 2 and 3, then run the sticker detector\n",
        "         and remap crop coordinates back to the original image frame.\n",
        "\n",
        "    The pipeline is designed to be compatible with :class:`YOLODetector` for\n",
        "    both the guide and sticker stages.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        detector: BaseDetector,\n",
        "        guide: BaseDetector,\n",
        "        coco_split_dir,\n",
        "        conf=[0.5, 0.5],\n",
        "        iou=[0.5, 0.5],\n",
        "        input_size=(960, 544),\n",
        "        seed=30,\n",
        "    ):\n",
        "        \"\"\"Initialize the GuidedPipeline.\n",
        "\n",
        "        Args:\n",
        "            detector: Sticker detector used in the second stage. Must implement\n",
        "                :class:`BaseDetector` and return sticker predictions in the\n",
        "                format used by :meth:`BaseDetector.predict_sticker`.\n",
        "            guide: Guide or windshield detector used in the first stage. Must\n",
        "                implement :class:`BaseDetector` and return windshields in the\n",
        "                format used by :meth:`BaseDetector.predict_windshield`.\n",
        "            coco_split_dir: Path to the COCO style split directory for this\n",
        "                dataset. This is typically used by external evaluation utilities\n",
        "                such as :func:`evaluate_fn`.\n",
        "            conf: Two element list of confidence thresholds\n",
        "                ``[sticker_conf, windshield_conf]`` used during prediction.\n",
        "            iou: Two element list of IoU thresholds\n",
        "                ``[sticker_iou, windshield_iou]`` used during prediction.\n",
        "            input_size: Input resolution (width, height) that can be used by\n",
        "                detectors. This value is stored but not enforced by the\n",
        "                pipeline itself.\n",
        "            seed: Seed for reproducibility. Forwarded to detector training\n",
        "                calls where supported.\n",
        "        \"\"\"\n",
        "        self.detector = detector\n",
        "        self.guide = guide\n",
        "        self.coco_split_dir = coco_split_dir\n",
        "        self.conf = conf\n",
        "        self.iou = iou\n",
        "        self.input_size = input_size\n",
        "        self.seed = seed\n",
        "\n",
        "    ############################\n",
        "    # Crop dataset utilities\n",
        "    ############################\n",
        "\n",
        "    def delete_folder_if_exists(self, folder_path):\n",
        "        \"\"\"Delete a folder and all its contents if it exists.\n",
        "\n",
        "        Args:\n",
        "            folder_path: Path to the folder to delete. This can be a string or\n",
        "                a :class:`pathlib.Path` object.\n",
        "\n",
        "        Side effects:\n",
        "            Logs a message and removes the directory recursively if it exists.\n",
        "        \"\"\"\n",
        "        path = Path(folder_path)\n",
        "        if path.exists() and path.is_dir():\n",
        "            print(f\"Deleting existing folder: {path}\")\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "    def _yolo_line_to_xyxy(self, line: str, W: int, H: int):\n",
        "        \"\"\"Convert a YOLO label line to pixel coordinates in xyxy format.\n",
        "\n",
        "        YOLO label lines have the form:\n",
        "\n",
        "            cls cx cy w h\n",
        "\n",
        "        where all coordinates are normalized relative to image width and height.\n",
        "\n",
        "        Args:\n",
        "            line: A single label line from a YOLO ``.txt`` file.\n",
        "            W: Image width in pixels.\n",
        "            H: Image height in pixels.\n",
        "\n",
        "        Returns:\n",
        "            Tuple ``(cls, x1, y1, x2, y2)`` where ``cls`` is the integer class\n",
        "            id and ``x1, y1, x2, y2`` are the bounding box coordinates in pixels.\n",
        "        \"\"\"\n",
        "        # \"cls cx cy w h\" (normalized) -> pixel xyxy\n",
        "        c, cx, cy, w, h = map(float, line.split())\n",
        "        bw, bh = w * W, h * H\n",
        "        x1 = (cx * W) - bw / 2\n",
        "        y1 = (cy * H) - bh / 2\n",
        "        x2 = x1 + bw\n",
        "        y2 = y1 + bh\n",
        "        return int(c), x1, y1, x2, y2\n",
        "\n",
        "    def _xyxy_to_yolo_line(self, x1, y1, x2, y2, W, H, cls=0):\n",
        "        \"\"\"Convert a pixel xyxy bounding box to a YOLO label line.\n",
        "\n",
        "        Args:\n",
        "            x1: Left coordinate of the bounding box in pixels.\n",
        "            y1: Top coordinate of the bounding box in pixels.\n",
        "            x2: Right coordinate of the bounding box in pixels.\n",
        "            y2: Bottom coordinate of the bounding box in pixels.\n",
        "            W: Image width in pixels.\n",
        "            H: Image height in pixels.\n",
        "            cls: Integer class id to write.\n",
        "\n",
        "        Returns:\n",
        "            A YOLO label line as a string in the format:\n",
        "\n",
        "                \"cls cx cy w h\\\\n\"\n",
        "\n",
        "            where ``cx, cy, w, h`` are normalized to the range [0, 1]. If the\n",
        "            box is degenerate or has non positive width or height, returns\n",
        "            ``None``.\n",
        "        \"\"\"\n",
        "        # pixel xyxy -> \"cls cx cy w h\" normalized to W,H\n",
        "        bw = max(0.0, x2 - x1)\n",
        "        bh = max(0.0, y2 - y1)\n",
        "        if bw <= 0 or bh <= 0:\n",
        "            return None\n",
        "        cx = (x1 + x2) / 2.0 / W\n",
        "        cy = (y1 + y2) / 2.0 / H\n",
        "        w = bw / W\n",
        "        h = bh / H\n",
        "        # clip to [0,1] just in case\n",
        "        cx = min(max(cx, 0.0), 1.0)\n",
        "        cy = min(max(cy, 0.0), 1.0)\n",
        "        w = min(max(w, 0.0), 1.0)\n",
        "        h = min(max(h, 0.0), 1.0)\n",
        "        if w <= 0 or h <= 0:\n",
        "            return None\n",
        "        return f\"{int(cls)} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n\"\n",
        "\n",
        "    def _label_path_for_image(self, img_path: Path) -> Path:\n",
        "        \"\"\"Return the YOLO label path corresponding to an image path.\n",
        "\n",
        "        This helper expects the standard YOLO layout where labels are stored\n",
        "        in a sibling ``labels`` directory:\n",
        "\n",
        "            .../<split>/images/<name>.jpg\n",
        "            .../<split>/labels/<name>.txt\n",
        "\n",
        "        Args:\n",
        "            img_path: Path to an image in the ``images`` directory.\n",
        "\n",
        "        Returns:\n",
        "            Path to the corresponding label file in the ``labels`` directory.\n",
        "        \"\"\"\n",
        "        return img_path.parent.parent / \"labels\" / (img_path.stem + \".txt\")\n",
        "\n",
        "    def _resolve_split_dir(self, data_yaml_path: Path, split_key: str) -> Path | None:\n",
        "        \"\"\"Resolve the directory path for a dataset split from a data YAML.\n",
        "\n",
        "        This method reads the given data YAML and looks up the value under the\n",
        "        specified split key (for example ``\"train\"``, ``\"val\"``, or ``\"test\"``).\n",
        "        If the path in the YAML is relative, it is resolved relative to the\n",
        "        directory containing the YAML file.\n",
        "\n",
        "        Args:\n",
        "            data_yaml_path: Path to the data YAML file.\n",
        "            split_key: Key of the split to resolve, for example ``\"test\"``.\n",
        "\n",
        "        Returns:\n",
        "            A :class:`Path` to the split directory, or ``None`` if the split\n",
        "            key is missing or empty.\n",
        "        \"\"\"\n",
        "        y = yaml.safe_load(data_yaml_path.read_text())\n",
        "        rel = y.get(split_key)\n",
        "        if not rel:\n",
        "            return None\n",
        "        p = Path(rel)\n",
        "        return p if p.is_absolute() else (data_yaml_path.parent / p)\n",
        "\n",
        "    def build_sticker_crop_dataset(\n",
        "        self,\n",
        "        cropped_windshields,\n",
        "        data_yaml,\n",
        "        class_needed: str = 'car-sticker',\n",
        "        out_root: str = \"/content/sticker_crops\",\n",
        "    ):\n",
        "        \"\"\"Build a YOLO sticker crop dataset from cropped windshield regions.\n",
        "\n",
        "        This method constructs a new YOLO dataset rooted at ``out_root`` with\n",
        "        the following structure:\n",
        "\n",
        "            out_root/\n",
        "                train/\n",
        "                    images/\n",
        "                    labels/\n",
        "                test/\n",
        "                    images/\n",
        "                    labels/\n",
        "                data.yaml\n",
        "\n",
        "        The train split is populated from the provided ``cropped_windshields``\n",
        "        list, while the test split is copied from the original dataset defined\n",
        "        in ``data_yaml`` (if a test split exists there).\n",
        "\n",
        "        Bounding boxes from the original labels are intersected with each crop\n",
        "        and remapped into crop coordinates, then written in YOLO format.\n",
        "\n",
        "        Args:\n",
        "            cropped_windshields: Iterable of crop records. Each record is\n",
        "                expected to be a dictionary with keys:\n",
        "\n",
        "                  * ``\"crop\"``: Crop image as a NumPy array (H, W, C).\n",
        "                  * ``\"box\"``: Tuple ``(x1, y1, x2, y2)`` in original image\n",
        "                    pixel coordinates describing the crop region.\n",
        "                  * ``\"image_path\"``: Path to the original source image.\n",
        "\n",
        "            data_yaml: Path to the original YOLO data YAML file. Class names\n",
        "                and the original test split are read from this file.\n",
        "            class_needed: Logical class name used to identify sticker classes,\n",
        "                for example ``\"car-sticker\"``. The name is normalized to\n",
        "                lowercase and underscores before matching against the YAML\n",
        "                names.\n",
        "            out_root: Root directory where the new crop dataset should be\n",
        "                created. Existing contents at this path are deleted.\n",
        "\n",
        "        Returns:\n",
        "            str: Path to the newly created ``data.yaml`` file inside\n",
        "            ``out_root``.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If ``cropped_windshields`` is empty.\n",
        "        \"\"\"\n",
        "        import re  # local import is fine here\n",
        "\n",
        "        def _norm(s: str) -> str:\n",
        "            # normalize \"Car-Sticker\", \"car sticker\", etc. -> \"car_sticker\"\n",
        "            return re.sub(r'[^a-z0-9]+', '_', s.strip().lower())\n",
        "\n",
        "        self.delete_folder_if_exists(out_root)\n",
        "\n",
        "        out_root = Path(out_root)\n",
        "        (out_root / \"train/images\").mkdir(parents=True, exist_ok=True)\n",
        "        (out_root / \"train/labels\").mkdir(parents=True, exist_ok=True)\n",
        "        (out_root / \"test/images\").mkdir(parents=True, exist_ok=True)\n",
        "        (out_root / \"test/labels\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if not cropped_windshields:\n",
        "            raise ValueError(\"No cropped windshields provided.\")\n",
        "\n",
        "        data_yaml_path = Path(data_yaml)\n",
        "        y = yaml.safe_load(data_yaml_path.read_text())\n",
        "\n",
        "        # preserve class list/order\n",
        "        names_raw = y.get(\"names\")\n",
        "        if isinstance(names_raw, dict):\n",
        "            names = [names_raw[k] for k in sorted(map(int, names_raw.keys()))]\n",
        "            write_as_dict = True\n",
        "            id2name = {int(k): v for k, v in names_raw.items()}\n",
        "        else:\n",
        "            names = list(names_raw) if names_raw is not None else []\n",
        "            write_as_dict = False\n",
        "            id2name = {i: n for i, n in enumerate(names)}\n",
        "        nc = len(names)\n",
        "\n",
        "        # figure out which original class IDs correspond to \"car_sticker\"\n",
        "        wanted_names = {class_needed}\n",
        "        wanted_norm = {_norm(w) for w in wanted_names}\n",
        "        sticker_ids = {i for i, n in id2name.items() if _norm(str(n)) in wanted_norm}\n",
        "        if not sticker_ids:\n",
        "            # fallback if names do not exist in YAML\n",
        "            sticker_ids = {0}\n",
        "\n",
        "        # original test dir (copy as-is)\n",
        "        src_test_dir = self._resolve_split_dir(data_yaml_path, \"test\")\n",
        "\n",
        "        # cache original image sizes\n",
        "        shape_cache = {}\n",
        "        EPS = 1e-6\n",
        "        MIN_PX = 2\n",
        "\n",
        "        # filename bookkeeping to preserve names and handle multiple crops per image\n",
        "        # key: absolute original image path -> number of crops already written\n",
        "        crop_counts = {}\n",
        "\n",
        "        # write crops for TRAIN folder only\n",
        "        for it in cropped_windshields:\n",
        "            crop_img, (x1c, y1c, x2c, y2c), orig_img_path = it[\"crop\"], it[\"box\"], Path(it[\"image_path\"])\n",
        "\n",
        "            # base stem + extension from original\n",
        "            base_stem = orig_img_path.stem\n",
        "            ext = orig_img_path.suffix if orig_img_path.suffix else \".jpg\"\n",
        "\n",
        "            # pick filename: first crop keeps exact name, subsequent get suffix _c2, _c3, ...\n",
        "            n = crop_counts.get(orig_img_path, 0) + 1\n",
        "            crop_counts[orig_img_path] = n\n",
        "            if n == 1:\n",
        "                img_name = f\"{base_stem}{ext}\"\n",
        "            else:\n",
        "                img_name = f\"{base_stem}_c{n}{ext}\"\n",
        "\n",
        "            img_out = out_root / \"train/images\" / img_name\n",
        "            lbl_out = out_root / \"train/labels\" / (Path(img_name).stem + \".txt\")\n",
        "\n",
        "            # ensure uint8 3ch\n",
        "            arr = crop_img\n",
        "            if isinstance(arr, np.ndarray) and arr.dtype != np.uint8:\n",
        "                arr = arr.astype(np.uint8)\n",
        "            if arr.ndim == 2:\n",
        "                arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2BGR)\n",
        "            cv2.imwrite(str(img_out), arr)\n",
        "\n",
        "            # map GT labels that intersect crop\n",
        "            orig_lbl_path = self._label_path_for_image(orig_img_path)\n",
        "            crop_W = max(EPS, (x2c - x1c))\n",
        "            crop_H = max(EPS, (y2c - y1c))\n",
        "            lines_out = []\n",
        "\n",
        "            if orig_lbl_path.exists():\n",
        "                if orig_img_path not in shape_cache:\n",
        "                    im = cv2.imread(str(orig_img_path))\n",
        "                    shape_cache[orig_img_path] = None if im is None else (im.shape[1], im.shape[0])  # (W,H)\n",
        "                shape = shape_cache.get(orig_img_path)\n",
        "                if shape is not None:\n",
        "                    W, H = shape\n",
        "                    with open(orig_lbl_path, \"r\") as f:\n",
        "                        for line in f:\n",
        "                            line = line.strip()\n",
        "                            if not line:\n",
        "                                continue\n",
        "                            cls, bx1, by1, bx2, by2 = self._yolo_line_to_xyxy(line, W, H)\n",
        "\n",
        "                            # intersect with crop (original coords)\n",
        "                            ix1 = max(bx1, x1c)\n",
        "                            iy1 = max(by1, y1c)\n",
        "                            ix2 = min(bx2, x2c)\n",
        "                            iy2 = min(by2, y2c)\n",
        "                            if ix2 - ix1 < MIN_PX or iy2 - iy1 < MIN_PX:\n",
        "                                continue  # no overlap / too tiny\n",
        "\n",
        "                            # map to crop-local\n",
        "                            cx1 = ix1 - x1c\n",
        "                            cy1 = iy1 - y1c\n",
        "                            cx2 = ix2 - x1c\n",
        "                            cy2 = iy2 - y1c\n",
        "\n",
        "                            yline = self._xyxy_to_yolo_line(\n",
        "                                cx1, cy1, cx2, cy2, crop_W, crop_H, cls=int(cls)\n",
        "                            )\n",
        "                            if yline:\n",
        "                                lines_out.append(yline)\n",
        "\n",
        "            with open(lbl_out, \"w\") as f:\n",
        "                for l in lines_out:\n",
        "                    f.write(l)\n",
        "\n",
        "        # copy ORIGINAL TEST split as-is\n",
        "        if src_test_dir and (src_test_dir / \"images\").exists():\n",
        "            for img in (src_test_dir / \"images\").glob(\"*.*\"):\n",
        "                if img.suffix.lower().lstrip(\".\") in {\n",
        "                    'bmp', 'mpo', 'jpg', 'pfm', 'tif', 'tiff',\n",
        "                    'png', 'webp', 'jpeg', 'dng', 'heic'\n",
        "                }:\n",
        "                    dst_img = out_root / \"test/images\" / img.name\n",
        "                    dst_lbl = out_root / \"test/labels\" / (img.stem + \".txt\")\n",
        "                    shutil.copy2(img, dst_img)\n",
        "                    lbl = src_test_dir / \"labels\" / (img.stem + \".txt\")\n",
        "                    if lbl.exists():\n",
        "                        shutil.copy2(lbl, dst_lbl)\n",
        "\n",
        "        # YAML: train and val both use train/, test uses test/ (if any)\n",
        "        yaml_lines = [\n",
        "            f\"path: {out_root}\",\n",
        "            \"train: train/images\",\n",
        "            \"val: train/images\",\n",
        "        ]\n",
        "        if (out_root / \"test/images\").exists() and any((out_root / \"test/images\").iterdir()):\n",
        "            yaml_lines.append(\"test: test/images\")\n",
        "        yaml_lines += [f\"nc: {nc}\", \"names:\"]\n",
        "        if write_as_dict:\n",
        "            for idx, name in enumerate(names):\n",
        "                yaml_lines.append(f\"  {idx}: {name}\")\n",
        "        else:\n",
        "            yaml_lines.append(\"  \" + str(names))\n",
        "        (out_root / \"data.yaml\").write_text(\"\\n\".join(yaml_lines) + \"\\n\")\n",
        "\n",
        "        print(\n",
        "            'Built crop dataset with original filenames (subsequent crops use _c2/_c3 suffixes). '\n",
        "            f\"Root: {out_root}\"\n",
        "        )\n",
        "        return str(out_root / \"data.yaml\")\n",
        "\n",
        "    ############################\n",
        "    # Core pipeline flow\n",
        "    ############################\n",
        "\n",
        "    def crop_windshields(self, predicted_ws):\n",
        "        \"\"\"Crop windshield regions from full images.\n",
        "\n",
        "        This function consumes the guide predictions returned by\n",
        "        :meth:`BaseDetector.predict_windshield` and extracts padded crops\n",
        "        around each guide bounding box.\n",
        "\n",
        "        Args:\n",
        "            predicted_ws: List of guide prediction entries, where each entry\n",
        "                has the form:\n",
        "\n",
        "                    {\n",
        "                        \"image_path\": \"<path/to/image.jpg>\",\n",
        "                        \"guides\": [\n",
        "                            {\n",
        "                                \"xyxy\": [x1, y1, x2, y2],\n",
        "                                \"conf\": float_confidence,\n",
        "                                \"cls\": class_id\n",
        "                            },\n",
        "                            ...\n",
        "                        ]\n",
        "                    }\n",
        "\n",
        "                If ``\"guides\"`` is empty, no crops are produced for that image.\n",
        "\n",
        "        Returns:\n",
        "            list[dict]: A list of crop records, each with the structure:\n",
        "\n",
        "                {\n",
        "                    \"image_path\": \"<original/image/path.jpg>\",\n",
        "                    \"crop\": np.ndarray,              # cropped BGR image\n",
        "                    \"box\": [x1c, y1c, x2c, y2c],     # crop box in original coords\n",
        "                    \"conf\": float_confidence         # guide confidence\n",
        "                }\n",
        "\n",
        "            Entries whose crops are empty are skipped.\n",
        "        \"\"\"\n",
        "        cropped_images = []\n",
        "        pad = 5\n",
        "        for item in tqdm(predicted_ws, desc=\"Cropping windshields\"):\n",
        "            img_path = item[\"image_path\"]\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            for g in item[\"guides\"]:\n",
        "                xy = g[\"xyxy\"]\n",
        "                conf = g.get(\"conf\", 0.0)\n",
        "\n",
        "                x1, y1, x2, y2 = map(int, xy[:4])\n",
        "\n",
        "                x1c = max(0, x1 - pad)\n",
        "                y1c = max(0, y1 - pad)\n",
        "                x2c = min(img.shape[1], x2 + pad)\n",
        "                y2c = min(img.shape[0], y2 + pad)\n",
        "                crop = img[y1c:y2c, x1c:x2c]\n",
        "                if crop.size == 0:\n",
        "                    continue\n",
        "\n",
        "                cropped_images.append({\n",
        "                    \"image_path\": img_path,\n",
        "                    \"crop\": crop,\n",
        "                    \"box\": [x1c, y1c, x2c, y2c],\n",
        "                    \"conf\": conf\n",
        "                })\n",
        "        return cropped_images\n",
        "\n",
        "    def _make_crop_index(self, cropped_windshields, out_root=\"/content/sticker_crops\"):\n",
        "        \"\"\"Build an index that maps crop file paths back to original images.\n",
        "\n",
        "        This helper reconstructs the filenames that will be written by\n",
        "        :meth:`build_sticker_crop_dataset` for the train split and builds a\n",
        "        mapping from crop path to the original image path and crop box\n",
        "        coordinates.\n",
        "\n",
        "        It assumes the following behavior:\n",
        "\n",
        "          * Crops are saved in ``out_root/train/images``.\n",
        "          * The first crop for an original image keeps the base name\n",
        "            ``<stem><ext>``.\n",
        "          * Subsequent crops for the same original image receive suffixes\n",
        "            ``_c2``, ``_c3``, and so on.\n",
        "\n",
        "        Args:\n",
        "            cropped_windshields: List of crop records produced by\n",
        "                :meth:`crop_windshields`.\n",
        "            out_root: Root directory where the sticker crop dataset is or will\n",
        "                be created.\n",
        "\n",
        "        Returns:\n",
        "            dict: A mapping from crop image path to metadata:\n",
        "\n",
        "                {\n",
        "                    \"<out_root>/train/images/<name>.jpg\": {\n",
        "                        \"orig_path\": \"<original/image/path.jpg>\",\n",
        "                        \"crop_box\": [x1c, y1c, x2c, y2c]\n",
        "                    },\n",
        "                    ...\n",
        "                }\n",
        "        \"\"\"\n",
        "        out_root = Path(out_root)\n",
        "        img_dir = out_root / \"train\" / \"images\"\n",
        "\n",
        "        crop_counts = {}\n",
        "        index = {}\n",
        "\n",
        "        for it in cropped_windshields:\n",
        "            orig_img_path = Path(it[\"image_path\"])\n",
        "            x1c, y1c, x2c, y2c = it[\"box\"]\n",
        "            base_stem = orig_img_path.stem\n",
        "            ext = orig_img_path.suffix if orig_img_path.suffix else \".jpg\"\n",
        "\n",
        "            n = crop_counts.get(orig_img_path, 0) + 1\n",
        "            crop_counts[orig_img_path] = n\n",
        "\n",
        "            if n == 1:\n",
        "                img_name = f\"{base_stem}{ext}\"\n",
        "            else:\n",
        "                img_name = f\"{base_stem}_c{n}{ext}\"\n",
        "\n",
        "            crop_path = str(img_dir / img_name)\n",
        "\n",
        "            index[crop_path] = {\n",
        "                \"orig_path\": str(orig_img_path),\n",
        "                \"crop_box\": [int(x1c), int(y1c), int(x2c), int(y2c)]\n",
        "            }\n",
        "\n",
        "        return index\n",
        "\n",
        "    def remap_sticker_predictions(self, sticker_preds, crop_index):\n",
        "        \"\"\"Remap crop space sticker predictions back to original image space.\n",
        "\n",
        "        This function takes sticker predictions produced by the second stage\n",
        "        detector and moves each bounding box from crop coordinates into the\n",
        "        coordinate frame of the original image, using the crop index built by\n",
        "        :meth:`_make_crop_index`.\n",
        "\n",
        "        Args:\n",
        "            sticker_preds: Sticker predictions returned by\n",
        "                :meth:`BaseDetector.predict_sticker`, typically with entries of\n",
        "                the form:\n",
        "\n",
        "                    {\n",
        "                        \"crop_path\": \"<path/to/crop.jpg>\",\n",
        "                        \"boxes\": [\n",
        "                            {\n",
        "                                \"xyxy\": [x1, y1, x2, y2],\n",
        "                                \"conf\": float_confidence,\n",
        "                                \"cls\": class_id\n",
        "                            },\n",
        "                            ...\n",
        "                        ]\n",
        "                    }\n",
        "\n",
        "            crop_index: Mapping produced by :meth:`_make_crop_index` where each\n",
        "                crop path is mapped to an original image path and crop box.\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping from original image path to a list of remapped boxes:\n",
        "\n",
        "                {\n",
        "                    \"<original/image/path.jpg>\": [\n",
        "                        {\n",
        "                            \"xyxy\": [X1, Y1, X2, Y2],   # original image coords\n",
        "                            \"conf\": float_confidence,\n",
        "                            \"cls\": class_id\n",
        "                        },\n",
        "                        ...\n",
        "                    ],\n",
        "                    ...\n",
        "                }\n",
        "\n",
        "            Entries whose crop paths are not found in ``crop_index`` are\n",
        "            silently skipped.\n",
        "        \"\"\"\n",
        "        remapped = {}\n",
        "\n",
        "        for item in sticker_preds:\n",
        "            crop_path = item[\"crop_path\"]\n",
        "            boxes = item[\"boxes\"]\n",
        "\n",
        "            if crop_path not in crop_index:\n",
        "                # silently skip if crop is not in index (for example test images not from crops)\n",
        "                continue\n",
        "\n",
        "            info = crop_index[crop_path]\n",
        "            orig_path = info[\"orig_path\"]\n",
        "            x1c, y1c, x2c, y2c = info[\"crop_box\"]\n",
        "\n",
        "            L = remapped.setdefault(orig_path, [])\n",
        "\n",
        "            for b in boxes:\n",
        "                x1, y1, x2, y2 = b[\"xyxy\"]\n",
        "                # shift from crop local to original image coords\n",
        "                X1 = float(x1) + x1c\n",
        "                Y1 = float(y1) + y1c\n",
        "                X2 = float(x2) + x1c\n",
        "                Y2 = float(y2) + y1c\n",
        "\n",
        "                nb = {\n",
        "                    \"xyxy\": [X1, Y1, X2, Y2],\n",
        "                    \"conf\": b.get(\"conf\", 0.0)\n",
        "                }\n",
        "                if \"cls\" in b:\n",
        "                    nb[\"cls\"] = b[\"cls\"]\n",
        "                L.append(nb)\n",
        "\n",
        "        return remapped\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        data_yaml,\n",
        "        epochs=[100, 100],\n",
        "        scheduled_epochs=[],\n",
        "        skip_windshield_train=False,\n",
        "        skip_sticker_train=False,\n",
        "    ):\n",
        "        \"\"\"Train the guide and sticker detectors in a two stage procedure.\n",
        "\n",
        "        Stage 1 - guide or windshield detector:\n",
        "\n",
        "          1. Train ``self.guide`` on the full dataset for the windshield class\n",
        "             (class index 1 by convention).\n",
        "          2. Predict windshields on the train split using the trained guide.\n",
        "          3. Crop predicted windshields with :meth:`crop_windshields`.\n",
        "          4. Build a sticker crop dataset and YAML using\n",
        "             :meth:`build_sticker_crop_dataset`.\n",
        "\n",
        "        Stage 2 - sticker detector:\n",
        "\n",
        "          1. Train ``self.detector`` on the sticker crop dataset for the\n",
        "             sticker class (class index 0 by convention).\n",
        "\n",
        "        Either stage can be skipped if a pretrained model or prebuilt dataset\n",
        "        already exists.\n",
        "\n",
        "        Args:\n",
        "            data_yaml: Path to the original YOLO data YAML file describing the\n",
        "                full dataset.\n",
        "            epochs: Two element list ``[sticker_epochs, windshield_epochs]``\n",
        "                controlling the number of training epochs for each stage.\n",
        "            scheduled_epochs: Reserved for integration with schedulers such as\n",
        "                :class:`ReduceOnPlateauMAP50_WithDetectorNNClone`. Not used\n",
        "                directly in this method but can be forwarded through\n",
        "                ``**kwargs`` in custom implementations.\n",
        "            skip_windshield_train: If ``True``, skip training the guide\n",
        "                detector and assume ``data_yaml`` already points to a sticker\n",
        "                crop dataset.\n",
        "            skip_sticker_train: If ``True``, skip training the sticker\n",
        "                detector.\n",
        "\n",
        "        Returns:\n",
        "            None. Training side effects are handled by the detector\n",
        "            implementations.\n",
        "        \"\"\"\n",
        "        if not skip_windshield_train:\n",
        "            print(\"Training Windshield Detector...\")\n",
        "            windshield_ap = self.guide.train_model(\n",
        "                data_yaml=data_yaml,\n",
        "                classes=[1],\n",
        "                epochs=epochs[1],\n",
        "                conf=self.conf[1], iou=self.iou[1],\n",
        "                seed=self.seed,\n",
        "                yolo_names=['windshield']\n",
        "            )\n",
        "\n",
        "            print(\"Predicting Windshields for training...\\n\\n\")\n",
        "            predicted_ws = self.guide.predict_windshield(\n",
        "                data_yaml=data_yaml,\n",
        "                conf=self.conf[1], iou=self.iou[1],\n",
        "                classes=[1],\n",
        "                split='train'\n",
        "            )\n",
        "\n",
        "            print(\"Cropping windshields for sticker training...\")\n",
        "            cropped_windshields = self.crop_windshields(predicted_ws)\n",
        "\n",
        "            # display 1 image\n",
        "            if len(cropped_windshields) > 0:\n",
        "                first_crop = cropped_windshields[0][\"crop\"]\n",
        "                plt.figure(figsize=(6, 6))\n",
        "                plt.imshow(cv2.cvtColor(first_crop, cv2.COLOR_BGR2RGB))\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(\"First cropped windshield\")\n",
        "                plt.show()\n",
        "            else:\n",
        "                print(\"No windshields were cropped.\")\n",
        "\n",
        "            print(\"\\n\\nBuilding Dataset...\")\n",
        "            sticker_yaml = self.build_sticker_crop_dataset(cropped_windshields, data_yaml)\n",
        "            print('\\n\\nDataset Built Successfully.\\n')\n",
        "\n",
        "        else:\n",
        "            sticker_yaml = data_yaml\n",
        "            print(\"Skipping windshield training.\")\n",
        "\n",
        "        if not skip_sticker_train:\n",
        "            print(\"Training Sticker Detector...\")\n",
        "            self.detector.train_model(\n",
        "                data_yaml=sticker_yaml,\n",
        "                classes=[0],\n",
        "                epochs=epochs[0],\n",
        "                conf=self.conf[0], iou=self.iou[0],\n",
        "                seed=self.seed,\n",
        "                yolo_names=['car-sticker']\n",
        "            )\n",
        "\n",
        "    def predict(self, data_yaml, **kwargs):\n",
        "        \"\"\"Run the full two stage pipeline at inference time.\n",
        "\n",
        "        The prediction flow is:\n",
        "\n",
        "          1. Use ``self.guide.predict_windshield`` on the test split of\n",
        "             ``data_yaml`` to obtain guide boxes for each full image.\n",
        "          2. Crop the predicted windshields with :meth:`crop_windshields`.\n",
        "          3. Build a temporary sticker crop dataset and YAML via\n",
        "             :meth:`build_sticker_crop_dataset`.\n",
        "          4. Run ``self.detector.predict_sticker`` on the train split of the\n",
        "             crop dataset (which contains all sticker crops).\n",
        "          5. Remap crop space sticker predictions back to original image\n",
        "             coordinates with :meth:`remap_sticker_predictions`.\n",
        "          6. Visualize a small subset of results with\n",
        "             :meth:`visualize_first5`.\n",
        "\n",
        "        Args:\n",
        "            data_yaml: Path to the original YOLO data YAML file describing the\n",
        "                full dataset.\n",
        "            **kwargs: Reserved for future options. Currently not used but can\n",
        "                be wired to detectors if needed.\n",
        "\n",
        "        Returns:\n",
        "            dict: Remapped sticker predictions in the format produced by\n",
        "            :meth:`remap_sticker_predictions`, that is:\n",
        "\n",
        "                {\n",
        "                    \"<original/image/path.jpg>\": [\n",
        "                        {\n",
        "                            \"xyxy\": [X1, Y1, X2, Y2],\n",
        "                            \"conf\": float_confidence,\n",
        "                            \"cls\": class_id\n",
        "                        },\n",
        "                        ...\n",
        "                    ],\n",
        "                    ...\n",
        "                }\n",
        "        \"\"\"\n",
        "        print(\"\\n\\nPredicting windshields...\")\n",
        "        predicted_ws = self.guide.predict_windshield(\n",
        "            data_yaml=data_yaml,\n",
        "            classes=[1],\n",
        "            conf=self.conf[1], iou=self.iou[1],\n",
        "            split='test'\n",
        "        )\n",
        "\n",
        "        print(\"\\n\\nCropping windshields...\")\n",
        "        windshield_results = self.crop_windshields(predicted_ws)\n",
        "\n",
        "        # display 1 image\n",
        "        if len(windshield_results) > 0:\n",
        "            first_crop = windshield_results[0][\"crop\"]\n",
        "            plt.figure(figsize=(6, 6))\n",
        "            plt.imshow(cv2.cvtColor(first_crop, cv2.COLOR_BGR2RGB))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(\"First cropped windshield\")\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            print(\"No windshields were cropped.\")\n",
        "\n",
        "        print(\"\\n\\nBuilding Dataset...\")\n",
        "        sticker_yaml = self.build_sticker_crop_dataset(\n",
        "            windshield_results,\n",
        "            data_yaml\n",
        "        )\n",
        "        print('\\n\\nDataset Built Successfully.\\n')\n",
        "\n",
        "        print(\"\\n\\nPredicting Stickers...\")\n",
        "        # split = train because sticker_yaml has cropped images saved in train folder\n",
        "        sticker_preds = self.detector.predict_sticker(\n",
        "            data_yaml=sticker_yaml,\n",
        "            conf=self.conf[0], iou=self.iou[0],\n",
        "            classes=[0],\n",
        "            split='train'\n",
        "        )\n",
        "\n",
        "        print(\"Sticker Detection Complete.\")\n",
        "\n",
        "        print(\"\\n\\nRemapping Sticker Predictions...\")\n",
        "        # build crop index using the SAME windshield_results and out_root used in build_sticker_crop_dataset\n",
        "        crop_index = self._make_crop_index(windshield_results, out_root=\"/content/sticker_crops\")\n",
        "\n",
        "        # remap crop space predictions to original image coordinates\n",
        "        remapped = self.remap_sticker_predictions(sticker_preds, crop_index)\n",
        "\n",
        "        print(\"\\n\\nRemapping Complete...\")\n",
        "\n",
        "        # visualize first 5 originals with remapped predictions\n",
        "        self.visualize_first5(remapped, max_show=5)\n",
        "\n",
        "        return remapped\n",
        "\n",
        "    def visualize_first5(self, remapped_preds, max_show=5, thickness=2):\n",
        "        \"\"\"Visualize a subset of remapped predictions against ground truth.\n",
        "\n",
        "        For up to ``max_show`` original images, this function:\n",
        "\n",
        "          * Loads the original image.\n",
        "          * Draws ground truth boxes from the matching YOLO label file\n",
        "            (class 0 only) in blue.\n",
        "          * Draws remapped predictions in green, labeled with confidence and\n",
        "            optional class id.\n",
        "          * Shows the result with Matplotlib.\n",
        "\n",
        "        Args:\n",
        "            remapped_preds: Dictionary of remapped predictions returned by\n",
        "                :meth:`predict`.\n",
        "            max_show: Maximum number of images to visualize.\n",
        "            thickness: Line thickness in pixels for the drawn rectangles.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        shown = 0\n",
        "        for orig_path, boxes in remapped_preds.items():\n",
        "            if shown >= max_show:\n",
        "                break\n",
        "\n",
        "            img = cv2.imread(orig_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            H, W = img.shape[:2]\n",
        "\n",
        "            # draw GT from YOLO label file (blue)\n",
        "            gt_count = 0\n",
        "            p = Path(orig_path)\n",
        "            lbl_path = p.parent.parent / \"labels\" / (p.stem + \".txt\")  # .../<split>/labels/<name>.txt\n",
        "            if lbl_path.exists():\n",
        "                with open(lbl_path, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) != 5:\n",
        "                            continue\n",
        "                        c, cx, cy, w, h = map(float, parts)   # cls cx cy w h  (normalized)\n",
        "\n",
        "                        # filter: only show class 0 GT\n",
        "                        if int(c) != 0:\n",
        "                            continue\n",
        "\n",
        "                        bw, bh = w * W, h * H\n",
        "                        x1 = int(cx * W - bw / 2.0)\n",
        "                        y1 = int(cy * H - bh / 2.0)\n",
        "                        x2 = int(x1 + bw)\n",
        "                        y2 = int(y1 + bh)\n",
        "                        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), thickness)  # blue = GT\n",
        "                        cv2.putText(\n",
        "                            img, f\"GT c{int(c)}\",\n",
        "                            (x1, max(0, y1 - 5)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                            (255, 0, 0), 1, cv2.LINE_AA\n",
        "                        )\n",
        "                        gt_count += 1\n",
        "\n",
        "            # draw predictions (green)\n",
        "            pred_count = 0\n",
        "            for b in boxes:\n",
        "                x1, y1, x2, y2 = map(int, b[\"xyxy\"])\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), thickness)  # green = Pred\n",
        "                label = f\"{b.get('conf', 0):.2f}\"\n",
        "                if \"cls\" in b:\n",
        "                    label = f\"c{b['cls']}:{label}\"\n",
        "                cv2.putText(\n",
        "                    img, label,\n",
        "                    (x1, max(0, y1 - 5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                    (0, 255, 0), 1, cv2.LINE_AA\n",
        "                )\n",
        "                pred_count += 1\n",
        "\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{Path(orig_path).name} | preds: {pred_count} | gt: {gt_count}\")\n",
        "            plt.show()\n",
        "\n",
        "            shown += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSOc2QkBSF3p"
      },
      "source": [
        "### EVAL FN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i9R-dHbBunpC"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json, time\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "def evaluate_fn(\n",
        "    coco_split_dir,\n",
        "    predictions,\n",
        "    class_map=None,\n",
        "    yolo_names=None,\n",
        "    save_dir=\"./coco_eval_out\",\n",
        "    target_size=(1200, 800),\n",
        "):\n",
        "    \"\"\"Evaluate detector predictions on a COCO style dataset using AP and AR.\n",
        "\n",
        "    This function converts predictions from the Guided Pipeline format into\n",
        "    COCO style detection results, rescales both ground truth and predictions\n",
        "    to a fixed reference size, and runs COCOeval to compute standard metrics.\n",
        "\n",
        "    The function is designed to work with outputs produced by\n",
        "    :meth:`BaseDetector.predict_sticker` or remapped predictions from\n",
        "    :meth:`GuidedPipeline.remap_sticker_predictions`.\n",
        "\n",
        "    Args:\n",
        "        coco_split_dir: Path to a COCO style split directory containing\n",
        "            ``_annotations.coco.json`` and the corresponding images.\n",
        "        predictions: Detector predictions in one of two formats:\n",
        "\n",
        "              dict:\n",
        "                  {\n",
        "                      \"<image_path>\": [\n",
        "                          {\n",
        "                              \"xyxy\": [x1, y1, x2, y2],\n",
        "                              \"conf\": float_confidence,\n",
        "                              \"cls\": class_id\n",
        "                          },\n",
        "                          ...\n",
        "                      ],\n",
        "                      ...\n",
        "                  }\n",
        "\n",
        "              list:\n",
        "                  [\n",
        "                      {\n",
        "                          \"image_path\" or \"orig_path\" or \"crop_path\": \"<path>\",\n",
        "                          \"detections\" or \"boxes\": [\n",
        "                              {\n",
        "                                  \"xyxy\": [x1, y1, x2, y2],\n",
        "                                  \"conf\": float_confidence,\n",
        "                                  \"cls\": class_id\n",
        "                              },\n",
        "                              ...\n",
        "                          ]\n",
        "                      },\n",
        "                      ...\n",
        "                  ]\n",
        "\n",
        "        class_map: Optional mapping from YOLO class index to COCO category id,\n",
        "            for example ``{0: 1}``. If ``None``, the mapping is inferred from\n",
        "            ``yolo_names`` and the COCO categories where possible.\n",
        "        yolo_names: Optional list of class names in YOLO order, used to infer\n",
        "            ``class_map`` when it is not provided.\n",
        "        save_dir: Output directory where the temporary COCO detection JSON\n",
        "            will be written.\n",
        "        target_size: Tuple ``(width, height)`` that specifies the reference\n",
        "            resolution. Both GT boxes and prediction boxes are scaled into\n",
        "            this size before evaluation. This is useful to evaluate models\n",
        "            that resize images internally.\n",
        "\n",
        "    Returns:\n",
        "        float: AP@0.50 (COCO mAP50) from ``COCOeval.stats[1]``.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a class mapping cannot be inferred for a multi class\n",
        "            COCO dataset.\n",
        "        TypeError: If ``predictions`` is neither a dict nor a list.\n",
        "    \"\"\"\n",
        "    coco_split_dir = Path(coco_split_dir)\n",
        "    ann_path = coco_split_dir / \"_annotations.coco.json\"\n",
        "\n",
        "    # ---- load COCO GT ----\n",
        "    cocoGt = COCO(str(ann_path))\n",
        "    img_ids = cocoGt.getImgIds()\n",
        "    imgs = cocoGt.loadImgs(img_ids)\n",
        "\n",
        "    # map by basename\n",
        "    basename_to_imgid = {Path(im[\"file_name\"]).name: im[\"id\"] for im in imgs}\n",
        "\n",
        "    # categories\n",
        "    cat_ids = cocoGt.getCatIds()\n",
        "    cats = cocoGt.loadCats(cat_ids)\n",
        "    name_to_catid = {c[\"name\"]: c[\"id\"] for c in cats}\n",
        "\n",
        "    # ---- build class_map ----\n",
        "    if class_map is None:\n",
        "        if yolo_names:\n",
        "            tmp = {i: name_to_catid[nm] for i, nm in enumerate(yolo_names) if nm in name_to_catid}\n",
        "            if tmp: class_map = tmp\n",
        "        if class_map is None:\n",
        "            # fallback: try to find a single sticker like category\n",
        "            sticker_id = next((c[\"id\"] for c in cats if \"sticker\" in c[\"name\"].lower()), None)\n",
        "            if sticker_id: class_map = {0: sticker_id}\n",
        "    if class_map is None and len(cats) == 1:\n",
        "        class_map = {0: cats[0][\"id\"]}\n",
        "    if class_map is None:\n",
        "        raise ValueError(\"[evaluate] Could not infer class_map on multi-class COCO.\")\n",
        "\n",
        "    eval_cat_ids = sorted(set(class_map.values()))\n",
        "    def to_cat_id(yolo_cls): return class_map.get(int(yolo_cls), eval_cat_ids[0])\n",
        "\n",
        "    target_w, target_h = target_size\n",
        "\n",
        "    # ---- scale COCO GT to target size ----\n",
        "    for img in imgs:\n",
        "        orig_w, orig_h = img[\"width\"], img[\"height\"]\n",
        "        scale_x, scale_y = target_w / orig_w, target_h / orig_h\n",
        "        ann_ids = cocoGt.getAnnIds(imgIds=[img[\"id\"]])\n",
        "        anns = cocoGt.loadAnns(ann_ids)\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            ann[\"bbox\"] = [x * scale_x, y * scale_y, w * scale_x, h * scale_y]\n",
        "\n",
        "    # ---- normalize predictions ----\n",
        "    norm_preds = []\n",
        "    if isinstance(predictions, dict):\n",
        "        for k, v in predictions.items():\n",
        "            norm_preds.append({\"image_path\": k, \"detections\": v})\n",
        "    elif isinstance(predictions, list):\n",
        "        for it in predictions:\n",
        "            dets = it.get(\"detections\", it.get(\"boxes\", []))\n",
        "            ip = it.get(\"orig_path\") or it.get(\"image_path\") or it.get(\"crop_path\")\n",
        "            norm_preds.append({\"image_path\": ip, \"detections\": dets})\n",
        "    else:\n",
        "        raise TypeError(\"predictions must be dict or list\")\n",
        "\n",
        "    # ---- build DT list (scale predictions to target_size) ----\n",
        "    dt_list = []\n",
        "    skipped = 0\n",
        "    for it in norm_preds:\n",
        "        ipath = it[\"image_path\"]\n",
        "        if not ipath: continue\n",
        "        base = Path(ipath).name\n",
        "        img_id = basename_to_imgid.get(base)\n",
        "        if img_id is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        img_info = next(im for im in imgs if im[\"id\"] == img_id)\n",
        "        orig_w, orig_h = img_info[\"width\"], img_info[\"height\"]\n",
        "        scale_x, scale_y = target_w / orig_w, target_h / orig_h\n",
        "\n",
        "        for d in it.get(\"detections\", []):\n",
        "            x1, y1, x2, y2 = d[\"xyxy\"]\n",
        "            # scale predictions from original image to target_size\n",
        "            x1, x2 = x1 * scale_x, x2 * scale_x\n",
        "            y1, y2 = y1 * scale_y, y2 * scale_y\n",
        "            w, h = max(0.0, x2 - x1), max(0.0, y2 - y1)\n",
        "            if w <= 0 or h <= 0: continue\n",
        "\n",
        "            dt_list.append({\n",
        "                \"image_id\": int(img_id),\n",
        "                \"category_id\": int(to_cat_id(d.get(\"cls\", 0))),\n",
        "                \"bbox\": [float(x1), float(y1), w, h],\n",
        "                \"score\": float(d.get(\"conf\", 0.0))\n",
        "            })\n",
        "\n",
        "    if skipped:\n",
        "        print(f\"[evaluate] Skipped {skipped} prediction entries not found in COCO GT by basename.\")\n",
        "\n",
        "    # ---- debug counts per eval category ----\n",
        "    for cid in eval_cat_ids:\n",
        "        n_gt = len(cocoGt.getAnnIds(catIds=[cid]))\n",
        "        n_dt = sum(1 for d in dt_list if d[\"category_id\"] == cid)\n",
        "        cname = next((c[\"name\"] for c in cats if c[\"id\"] == cid), str(cid))\n",
        "        print(f\"[evaluate] Category '{cname}' (id={cid}): GT={n_gt}, DT={n_dt}\")\n",
        "\n",
        "    # ---- run COCOeval ----\n",
        "    save_dir = Path(save_dir); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    dt_json = save_dir / f\"coco_dt_{int(time.time())}.json\"\n",
        "    dt_json.write_text(json.dumps(dt_list))\n",
        "\n",
        "    cocoDt = cocoGt.loadRes(str(dt_json)) if len(dt_list) > 0 else COCO()\n",
        "    E = COCOeval(cocoGt, cocoDt, iouType=\"bbox\")\n",
        "    E.params.imgIds = img_ids\n",
        "    E.params.catIds = eval_cat_ids\n",
        "    E.evaluate(); E.accumulate(); E.summarize()\n",
        "\n",
        "    stats = {k: float(E.stats[i]) for i, k in enumerate([\n",
        "        \"AP@[.5:.95]\", \"AP@0.50\", \"AP@0.75\", \"AP_small\", \"AP_medium\", \"AP_large\",\n",
        "        \"AR@1\", \"AR@10\", \"AR@100\", \"AR_small\", \"AR_medium\", \"AR_large\"\n",
        "    ])}\n",
        "\n",
        "    ap50 = float(E.stats[1])\n",
        "    print(\"COCO stats:\", stats)\n",
        "    return ap50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1A-pkuIBrve"
      },
      "source": [
        "### ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xNQjBJ557qct"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Optional, Dict, List\n",
        "import copy, math\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ReduceOnPlateauMAP50_WithDetectorNNClone:\n",
        "    \"\"\"External AP50 based learning rate scheduler for Ultralytics YOLO.\n",
        "\n",
        "    This callback evaluates AP at IoU 0.50 (AP50) using an external detector\n",
        "    clone and a COCO style evaluation function, and reduces the learning rate\n",
        "    when performance stalls.\n",
        "\n",
        "    Workflow at selected epochs:\n",
        "\n",
        "    1. Copy the current network weights from `owner.model` into a new detector\n",
        "       instance from `detector_factory`.\n",
        "    2. Run `clone_det.predict_sticker(**predict_kwargs)` on a chosen split\n",
        "       (usually the train split).\n",
        "    3. Compute AP50 with `evaluate_fn` on a COCO split at `coco_split_dir`.\n",
        "    4. If AP50 has not improved for `patience` epochs (after activation),\n",
        "       reduce learning rate by `factor` while respecting `min_lr`.\n",
        "    5. Optionally run a second evaluation on a test split and log results.\n",
        "\n",
        "    This class is meant to be attached to Ultralytics callbacks:\n",
        "\n",
        "    * `model.add_callback(\"on_fit_epoch_end\", plateau_cb)` calls `__call__`.\n",
        "    * `model.add_callback(\"on_train_epoch_start\", plateau_cb.on_train_epoch_start)`\n",
        "      enforces target learning rates at the start of each epoch.\n",
        "    * `model.add_callback(\"on_fit_end\", plateau_cb.on_fit_end)` and\n",
        "      `model.add_callback(\"on_train_end\", plateau_cb.on_train_end)` print\n",
        "      an evaluation summary.\n",
        "\n",
        "    It assumes `owner` follows the `YOLODetector` style interface and that the\n",
        "    clone has a compatible `predict_sticker` method that returns the standard\n",
        "    sticker prediction format for the Guided Pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    factor: float = 0.5\n",
        "    patience: int = 10\n",
        "    patience_after_first: Optional[int] = None\n",
        "    cooldown: int = 0\n",
        "    min_lr: float = 1e-6\n",
        "\n",
        "    warmup_epochs: int = 0\n",
        "    start_after_map: float = 0.0\n",
        "\n",
        "    use_plateau: bool = True\n",
        "\n",
        "    scheduled_epochs: List[int] = field(default_factory=list)\n",
        "    scheduled_factors: Dict[int, float] = field(default_factory=dict)\n",
        "    scheduled_set_lrs: Dict[int, float] = field(default_factory=dict)\n",
        "\n",
        "    owner: Any = None\n",
        "    detector_factory: Optional[Callable[[], Any]] = None\n",
        "    evaluate_fn: Optional[Callable[..., float]] = None\n",
        "    coco_split_dir: str = \"\"\n",
        "    yolo_names: Optional[List[str]] = None\n",
        "    predict_kwargs: Optional[Dict[str, Any]] = None\n",
        "    eval_every: int = 1\n",
        "    clone_device: Optional[str] = \"cpu\"\n",
        "    verbose: bool = True\n",
        "\n",
        "    test_eval_every: Optional[int] = 25\n",
        "    test_eval_start_epoch: int = 0\n",
        "    test_eval_epochs: Optional[List[int]] = None\n",
        "    test_predict_kwargs: Optional[Dict[str, Any]] = None\n",
        "    test_coco_split_dir: Optional[str] = None\n",
        "    test_ap_history: List[Dict[str, Any]] = field(default_factory=list)\n",
        "\n",
        "    _best: float = float(\"-inf\")\n",
        "    _bad: int = 0\n",
        "    _cool: int = 0\n",
        "    _active: bool = False\n",
        "\n",
        "    history: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    _reduced_flag: bool = False\n",
        "    _lr_after: Optional[List[float]] = None\n",
        "    _printed: bool = False\n",
        "    _target_lrs: Optional[List[float]] = None\n",
        "    _had_reduction: bool = False\n",
        "\n",
        "    _scheduled_applied: set = field(default_factory=set)\n",
        "\n",
        "    def __call__(self, trainer):\n",
        "        \"\"\"Ultralytics callback entry that runs at the end of each epoch.\n",
        "\n",
        "        At epochs that pass the warmup and `eval_every` conditions, this method:\n",
        "\n",
        "        1. Clones the current YOLO network from `owner.model` into a new\n",
        "           detector instance returned by `detector_factory`.\n",
        "        2. Runs `clone_det.predict_sticker(**predict_kwargs)` with gradients\n",
        "           disabled.\n",
        "        3. Computes AP50 using `evaluate_fn` on `coco_split_dir`.\n",
        "        4. Applies any scheduled LR events through `_maybe_scheduled_step`.\n",
        "        5. If plateau logic is active and AP50 has not improved for `patience`\n",
        "           epochs, reduces learning rate and starts a cooldown.\n",
        "        6. Optionally runs a test evaluation on a different split and logs\n",
        "           the AP50 in `test_ap_history`.\n",
        "        7. Logs epoch index, train loss snapshot, AP50 and LR values in\n",
        "           `history`.\n",
        "\n",
        "        Args:\n",
        "            trainer: Ultralytics trainer object that owns the optimizer and\n",
        "                training state.\n",
        "        \"\"\"\n",
        "        epoch = int(getattr(trainer, \"epoch\", 0))\n",
        "        if epoch < self.warmup_epochs:\n",
        "            return\n",
        "        if self.eval_every > 1 and (epoch % self.eval_every):\n",
        "            return\n",
        "        if not (self.owner and self.detector_factory and self.evaluate_fn):\n",
        "            return\n",
        "\n",
        "        self._reduced_flag = False\n",
        "        self._lr_after = None\n",
        "\n",
        "        train_loss = self._get_train_loss(trainer)\n",
        "\n",
        "        ultra_train = getattr(self.owner, \"model\", None)\n",
        "        if ultra_train is None:\n",
        "            return\n",
        "        nn_train = getattr(ultra_train, \"model\", ultra_train)\n",
        "\n",
        "        clone_det = self.detector_factory()\n",
        "        ultra_clone = getattr(clone_det, \"model\", None)\n",
        "        if ultra_clone is None:\n",
        "            return\n",
        "\n",
        "        nn_copied = copy.deepcopy(nn_train)\n",
        "        nn_copied.eval()\n",
        "        if self.clone_device:\n",
        "            nn_copied.to(self.clone_device)\n",
        "        for p in nn_copied.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "        if hasattr(ultra_clone, \"model\"):\n",
        "            ultra_clone.model = nn_copied\n",
        "        else:\n",
        "            clone_det.model = nn_copied\n",
        "\n",
        "        for attr in (\"names\", \"nc\", \"args\"):\n",
        "            if hasattr(ultra_train, attr) and hasattr(ultra_clone, attr):\n",
        "                try:\n",
        "                    setattr(ultra_clone, attr, copy.deepcopy(getattr(ultra_train, attr)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            preds = clone_det.predict_sticker(**(self.predict_kwargs or {}))\n",
        "\n",
        "        m50 = float(self.evaluate_fn(\n",
        "            coco_split_dir=self.coco_split_dir,\n",
        "            predictions=preds,\n",
        "            yolo_names=self.yolo_names\n",
        "        ) or 0.0)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[ExtEval:det-nn-clone] epoch {epoch} mAP50(main)={m50:.6f}\")\n",
        "\n",
        "        self._maybe_scheduled_step(trainer, epoch)\n",
        "\n",
        "        if self.use_plateau:\n",
        "            if not self._active:\n",
        "                if m50 > self.start_after_map:\n",
        "                    self._active = True\n",
        "                    self._best = m50\n",
        "                    self._bad = 0\n",
        "                    if self.verbose:\n",
        "                        print(f\"[Plateau] activated at epoch {epoch}: mAP50={m50:.6f}\")\n",
        "            else:\n",
        "                if m50 > self._best + 1e-12:\n",
        "                    self._best = m50\n",
        "                    self._bad = 0\n",
        "                    if self.verbose:\n",
        "                        print(f\"[Plateau] new best mAP50={m50:.6f} (epoch {epoch})\")\n",
        "                else:\n",
        "                    self._bad += 1\n",
        "                    if self._cool > 0:\n",
        "                        self._cool -= 1\n",
        "                    else:\n",
        "                        eff_patience = self.patience if not self._had_reduction else (self.patience_after_first or self.patience)\n",
        "                        if self._bad >= eff_patience:\n",
        "                            self._reduce_lr(trainer)\n",
        "                            self._bad = 0\n",
        "                            self._cool = self.cooldown\n",
        "\n",
        "        do_test = False\n",
        "        if self.test_eval_every and self.test_eval_every > 0:\n",
        "            start = int(self.test_eval_start_epoch or 0)\n",
        "            if epoch >= start and ((epoch - start) % self.test_eval_every == 0):\n",
        "                do_test = True\n",
        "        if self.test_eval_epochs:\n",
        "            if epoch in self.test_eval_epochs:\n",
        "                do_test = True\n",
        "\n",
        "        if do_test:\n",
        "            tk = dict(self.predict_kwargs or {})\n",
        "            tk[\"split\"] = \"test\"\n",
        "            if self.test_predict_kwargs:\n",
        "                tk.update(self.test_predict_kwargs)\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                preds_test = clone_det.predict_sticker(**tk)\n",
        "\n",
        "            test_dir = self.test_coco_split_dir or self.coco_split_dir\n",
        "            m50_test = float(self.evaluate_fn(\n",
        "                coco_split_dir=test_dir,\n",
        "                predictions=preds_test,\n",
        "                yolo_names=self.yolo_names\n",
        "            ) or 0.0)\n",
        "            self.test_ap_history.append({\"epoch\": epoch, \"ap50\": m50_test})\n",
        "            if self.verbose:\n",
        "                print(f\"[ExtEval:TEST] epoch {epoch} mAP50(test)={m50_test:.6f}\")\n",
        "\n",
        "        cur_lrs = self._lr_after if self._lr_after is not None else self._get_lrs(trainer)\n",
        "        self.history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"ap50\": m50,\n",
        "            \"lrs\": cur_lrs,\n",
        "            \"reduced\": self._reduced_flag,\n",
        "        })\n",
        "\n",
        "        total_epochs = self._get_total_epochs(trainer)\n",
        "        if total_epochs is not None and (epoch + 1) >= int(total_epochs) and not self._printed:\n",
        "            self._print_history()\n",
        "            self._printed = True\n",
        "\n",
        "        if cur_lrs is not None:\n",
        "            self._target_lrs = list(cur_lrs)\n",
        "\n",
        "    def on_train_epoch_start(self, trainer):\n",
        "        \"\"\"Ultralytics callback to enforce target learning rates.\n",
        "\n",
        "        This method should be registered as `on_train_epoch_start`. It applies\n",
        "        the current target learning rates stored in `_target_lrs` to the\n",
        "        trainer optimizer and scheduler before the new epoch begins.\n",
        "        \"\"\"\n",
        "        self._apply_target_lrs(trainer)\n",
        "\n",
        "    def _apply_target_lrs(self, trainer):\n",
        "        if not self._target_lrs:\n",
        "            return\n",
        "        opt = getattr(trainer, \"optimizer\", None)\n",
        "        if opt is None:\n",
        "            return\n",
        "        for pg, v in zip(opt.param_groups, self._target_lrs):\n",
        "            pg[\"lr\"] = float(v)\n",
        "            if \"initial_lr\" in pg:\n",
        "                pg[\"initial_lr\"] = float(v)\n",
        "        args = getattr(trainer, \"args\", None)\n",
        "        try:\n",
        "            if args is not None and hasattr(args, \"lr0\"):\n",
        "                setattr(args, \"lr0\", float(min(self._target_lrs)))\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if hasattr(trainer, \"lr0\"):\n",
        "                if isinstance(trainer.lr0, (list, tuple)):\n",
        "                    trainer.lr0 = list(self._target_lrs)\n",
        "                else:\n",
        "                    trainer.lr0 = float(min(self._target_lrs))\n",
        "        except Exception:\n",
        "            pass\n",
        "        sch = getattr(trainer, \"scheduler\", None)\n",
        "        if sch is not None and hasattr(sch, \"base_lrs\"):\n",
        "            try:\n",
        "                sch.base_lrs = [float(v) for v in self._target_lrs]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    def _maybe_scheduled_step(self, trainer, epoch: int):\n",
        "        if epoch in self._scheduled_applied:\n",
        "            return\n",
        "        if epoch in self.scheduled_set_lrs:\n",
        "            target = float(self.scheduled_set_lrs[epoch])\n",
        "            self._set_absolute_lr(trainer, target)\n",
        "            self._scheduled_applied.add(epoch)\n",
        "            if self.verbose:\n",
        "                print(f\"[Schedule] epoch {epoch}: set LR -> {target}\")\n",
        "            return\n",
        "        if (epoch in self.scheduled_epochs) or (epoch in self.scheduled_factors):\n",
        "            fac = float(self.scheduled_factors.get(epoch, self.factor))\n",
        "            self._reduce_lr(trainer, factor=fac)\n",
        "            self._scheduled_applied.add(epoch)\n",
        "            if self.verbose:\n",
        "                print(f\"[Schedule] epoch {epoch}: reduce LR by factor {fac}\")\n",
        "\n",
        "    def _set_absolute_lr(self, trainer, target_lr: float):\n",
        "        opt = getattr(trainer, \"optimizer\", None)\n",
        "        if opt is None:\n",
        "            return\n",
        "        new_vals = []\n",
        "        for pg in opt.param_groups:\n",
        "            pg[\"lr\"] = max(float(target_lr), self.min_lr)\n",
        "            new_vals.append(float(pg[\"lr\"]))\n",
        "        self._reduced_flag = True\n",
        "        self._lr_after = new_vals\n",
        "        self._target_lrs = list(new_vals)\n",
        "        self._had_reduction = True\n",
        "        self._apply_target_lrs(trainer)\n",
        "\n",
        "    def _reduce_lr(self, trainer, factor: Optional[float] = None):\n",
        "        if not hasattr(trainer, \"optimizer\") or trainer.optimizer is None:\n",
        "            return\n",
        "        fac = float(factor if factor is not None else self.factor)\n",
        "        new_vals = []\n",
        "        for pg in trainer.optimizer.param_groups:\n",
        "            old = float(pg.get(\"lr\", 0.0))\n",
        "            new = max(old * fac, self.min_lr)\n",
        "            if new < old - 1e-12:\n",
        "                pg[\"lr\"] = new\n",
        "            new_vals.append(pg.get(\"lr\", old))\n",
        "        self._reduced_flag = True\n",
        "        self._lr_after = new_vals\n",
        "        self._target_lrs = list(new_vals)\n",
        "        if not self._had_reduction:\n",
        "            self._had_reduction = True\n",
        "            if self.patience_after_first is not None:\n",
        "                self.patience = self.patience_after_first\n",
        "        self._apply_target_lrs(trainer)\n",
        "        if self.verbose:\n",
        "            print(f\"[Plateau] LR reduced -> {new_vals}\")\n",
        "\n",
        "    def _get_lrs(self, trainer):\n",
        "        opt = getattr(trainer, \"optimizer\", None)\n",
        "        if opt is None:\n",
        "            return None\n",
        "        try:\n",
        "            return [float(pg.get(\"lr\", 0.0)) for pg in opt.param_groups]\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _get_train_loss(self, trainer) -> Optional[float]:\n",
        "        for attr in (\"tloss\", \"train_loss\", \"loss\"):\n",
        "            if hasattr(trainer, attr):\n",
        "                v = getattr(trainer, attr)\n",
        "                try:\n",
        "                    if torch.is_tensor(v):\n",
        "                        return float(v.detach().cpu().item())\n",
        "                    return float(v)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if hasattr(trainer, \"loss_items\"):\n",
        "            try:\n",
        "                li = trainer.loss_items\n",
        "                if torch.is_tensor(li):\n",
        "                    li = li.detach().cpu().tolist()\n",
        "                return float(sum(map(float, li)))\n",
        "            except Exception:\n",
        "                pass\n",
        "        m = getattr(trainer, \"metrics\", None)\n",
        "        if isinstance(m, dict):\n",
        "            for k in (\"train/loss\", \"loss\", \"metrics/loss\"):\n",
        "                if k in m:\n",
        "                    try:\n",
        "                        return float(m[k])\n",
        "                    except Exception:\n",
        "                        pass\n",
        "        return None\n",
        "\n",
        "    def _get_total_epochs(self, trainer) -> Optional[int]:\n",
        "        if hasattr(trainer, \"epochs\"):\n",
        "            try:\n",
        "                return int(getattr(trainer, \"epochs\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "        args = getattr(trainer, \"args\", None)\n",
        "        if args is not None and hasattr(args, \"epochs\"):\n",
        "            try:\n",
        "                return int(getattr(args, \"epochs\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "    def _print_history(self):\n",
        "        \"\"\"Print a summary of AP50 and LR history and periodic test AP50.\n",
        "\n",
        "        This is called automatically at the end of training from `on_fit_end`\n",
        "        or `on_train_end` if it has not been printed yet.\n",
        "        \"\"\"\n",
        "        if not self.history:\n",
        "            print(\"[History] No eval records.\")\n",
        "            return\n",
        "        print(\"\\n==== Eval History (epoch | loss | AP50(main) | LRs | LR reduced?) ====\")\n",
        "        for r in self.history:\n",
        "            loss_s = \"NA\"\n",
        "            if r[\"train_loss\"] is not None and math.isfinite(r[\"train_loss\"]):\n",
        "                loss_s = f\"{r['train_loss']:.6f}\"\n",
        "            ap_s = f\"{r['ap50']:.6f}\"\n",
        "            lr_s = \"NA\" if r[\"lrs\"] is None else \"[\" + \", \".join(f\"{x:.6g}\" for x in r[\"lrs\"]) + \"]\"\n",
        "            red_s = \"Y\" if r[\"reduced\"] else \"N\"\n",
        "            print(f\"epoch {r['epoch']:03d} | loss={loss_s} | AP50={ap_s} | LRs={lr_s} | reduced={red_s}\")\n",
        "        print(\"================================================================\")\n",
        "\n",
        "        if self.test_ap_history:\n",
        "            best = max(self.test_ap_history, key=lambda d: d[\"ap50\"])\n",
        "            print(\"\\n==== Periodic TEST AP50 (every N epochs) ====\")\n",
        "            for t in self.test_ap_history:\n",
        "                print(f\"epoch {t['epoch']:03d} | AP50(test)={t['ap50']:.6f}\")\n",
        "            print(f\"Best TEST AP50={best['ap50']:.6f} at epoch {best['epoch']}\")\n",
        "            print(\"=============================================\\n\")\n",
        "        else:\n",
        "            print(\"\\n[TEST] No periodic test evaluations recorded.\\n\")\n",
        "\n",
        "    def on_fit_end(self, trainer):\n",
        "        \"\"\"Ultralytics callback that prints the eval history at the end of fit.\"\"\"\n",
        "        if not self._printed:\n",
        "            self._print_history()\n",
        "            self._printed = True\n",
        "\n",
        "    def on_train_end(self, trainer):\n",
        "        \"\"\"Ultralytics callback that prints the eval history at the end of train.\"\"\"\n",
        "        if not self._printed:\n",
        "            self._print_history()\n",
        "            self._printed = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mbmStbUSIdZ"
      },
      "source": [
        "### SCRIPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feE2QD8TWQYl"
      },
      "outputs": [],
      "source": [
        "\"\"\"Example script for training and evaluating the GuidedPipeline.\n",
        "\n",
        "This example shows how to:\n",
        "\n",
        "1. Set up dataset paths for YOLO and COCO.\n",
        "2. Instantiate two `YOLODetector` instances:\n",
        "   * `windshield_model` as the guide or windshield detector.\n",
        "   * `sticker_model` as the sticker detector.\n",
        "3. Build a `GuidedPipeline` that uses:\n",
        "   * `guide` to detect windshields on full images.\n",
        "   * `detector` to detect stickers inside cropped windshields.\n",
        "4. Train the two stage pipeline:\n",
        "   * Stage 1: Train the guide on windshields.\n",
        "   * Stage 2: Use predicted windshields to build a crop dataset and train the sticker detector.\n",
        "5. Run inference on the test split:\n",
        "   * Predict windshields.\n",
        "   * Crop them.\n",
        "   * Predict stickers on crops.\n",
        "   * Remap sticker predictions back to original image coordinates.\n",
        "6. Evaluate the final sticker detections with COCO metrics using `evaluate_fn`.\n",
        "\n",
        "Expected formats:\n",
        "\n",
        "* `GuidedPipeline.predict` returns a dict mapping original image paths to\n",
        "  a list of remapped sticker detections:\n",
        "\n",
        "    {\n",
        "        \"<original/image.jpg>\": [\n",
        "            {\n",
        "                \"xyxy\": [x1, y1, x2, y2],\n",
        "                \"conf\": float_confidence,\n",
        "                \"cls\": int_class_id\n",
        "            },\n",
        "            ...\n",
        "        ],\n",
        "        ...\n",
        "    }\n",
        "\n",
        "  This format is accepted directly by `evaluate_fn` as `predictions`.\n",
        "\n",
        "* `evaluate_fn` computes COCO metrics on the given COCO split directory and\n",
        "  returns AP at IoU 0.50 (AP50).\n",
        "\n",
        "To run the full pipeline:\n",
        "\n",
        "1. Adjust the paths for `DATA_YAML`, `COCO_SPLIT`, and `COCO_SPLIT_TEST` to\n",
        "   your environment.\n",
        "2. Ensure that `YOLODetector`, `GuidedPipeline`, and `evaluate_fn` are imported\n",
        "   or defined in the same module.\n",
        "3. Run this script. Training, prediction, and evaluation will execute in order.\n",
        "\"\"\"\n",
        "\n",
        "DATA_YAML = '/content/drive/MyDrive/Dataset/windshield/31shot_ws/data.yaml'\n",
        "COCO_SPLIT = '/content/drive/MyDrive/Dataset_COCO/windshields/31shot_COCO/train'\n",
        "COCO_SPLIT_TEST = '/content/drive/MyDrive/Dataset_COCO/windshields/31shot_COCO/test'\n",
        "\n",
        "# Create guide (windshield) and detector (sticker) models\n",
        "windshield_model = YOLODetector(pretrained='yolov8n.pt')\n",
        "sticker_model = YOLODetector(pretrained='yolov8n.pt')\n",
        "\n",
        "# Build the guided pipeline\n",
        "pipeline = GuidedPipeline(\n",
        "    detector=sticker_model,\n",
        "    guide=windshield_model,\n",
        "    coco_split_dir=COCO_SPLIT_TEST,\n",
        "    conf=[0.05, 0.7],     # [sticker_conf, windshield_conf]\n",
        "    iou=[0.5, 0.7],       # [sticker_iou, windshield_iou]\n",
        "    input_size=(800, 1200),\n",
        "    seed=30,\n",
        ")\n",
        "\n",
        "# Two stage training:\n",
        "#   1) Train windshield detector, crop predictions, build sticker dataset.\n",
        "#   2) Train sticker detector on cropped windshield patches.\n",
        "pipeline.train(\n",
        "    data_yaml=DATA_YAML,\n",
        "    epochs=[100, 100],     # [sticker_epochs, windshield_epochs]\n",
        "    scheduled_epochs=[],   # can be used together with ReduceOnPlateau scheduler\n",
        ")\n",
        "\n",
        "# Full two stage inference on the test split:\n",
        "#   1) Predict windshields.\n",
        "#   2) Crop windshields.\n",
        "#   3) Build a temporary sticker crop dataset.\n",
        "#   4) Predict stickers on the crops.\n",
        "#   5) Remap sticker predictions to original image coordinates.\n",
        "preds = pipeline.predict(\n",
        "    data_yaml=DATA_YAML,\n",
        ")\n",
        "\n",
        "# COCO evaluation of remapped sticker predictions on the test split.\n",
        "# `preds` is a dict mapping original image paths to detection lists,\n",
        "# which matches the expected input format of `evaluate_fn`.\n",
        "metrics = evaluate_fn(\n",
        "    coco_split_dir=COCO_SPLIT_TEST,\n",
        "    predictions=preds,\n",
        "    yolo_names=[\"car-sticker\"],\n",
        "    target_size=(1280, 800),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Gtd4JN_FVG"
      },
      "source": [
        "### OPTIONAL FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APancB9TNoPk"
      },
      "outputs": [],
      "source": [
        "import json, time, os, random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def evaluate_fn_tags_visualize(\n",
        "    coco_split_dir,\n",
        "    predictions,\n",
        "    class_map=None,\n",
        "    yolo_names=None,\n",
        "    save_dir=\"./coco_eval_out\",\n",
        "    target_size=(1280, 800),\n",
        "    tags_of_interest=(\"left\", \"right\", \"far\", \"near\"),\n",
        "    tags_coco_json=None,\n",
        "    # visualization params (matplotlib-based)\n",
        "    visualize_img_indices=None,\n",
        "    save_visuals=False,\n",
        "    visual_save_dir=None,\n",
        "    visual_conf_thresh=0.0,\n",
        "    visual_box_alpha=0.45,\n",
        "    visual_text_alpha=0.35,\n",
        "    vis_limit=10,\n",
        "    # toggle for side recall computation\n",
        "    side_recall_mode=\"manual\",  # \"manual\" or \"coco\"\n",
        "    recall_conf_thresh=None,     # used in manual mode only. None = no filter\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate detector outputs on a COCO style split with tag based breakdowns,\n",
        "    side aware metrics, and optional visualizations.\n",
        "\n",
        "    This function extends a standard COCO evaluation with:\n",
        "      * Overall AP and AR on the the full split.\n",
        "      * Per tag AP and AR for tags such as \"left\", \"right\", \"far\", \"near\".\n",
        "      * Side aware AP for left and right halves of the windshield, using the\n",
        "        largest windshield box in ground truth to define the midline.\n",
        "      * Per tag side recall at IoU 0.5 for left and right halves, plus an\n",
        "        overall recall that combines both sides.\n",
        "      * Optional matplotlib based visualizations that highlight:\n",
        "          - True positives in green.\n",
        "          - False positives in orange.\n",
        "          - Missed ground truth boxes in red.\n",
        "          - Windshield boxes in purple.\n",
        "        Visualizations can be shown inline or saved to disk.\n",
        "\n",
        "    The function expects predictions in one of these formats:\n",
        "\n",
        "      1) Dict indexed by image path:\n",
        "\n",
        "         {\n",
        "             \"<image_path>\": [\n",
        "                 {\"xyxy\": [x1, y1, x2, y2], \"conf\": float, \"cls\": int},\n",
        "                 ...\n",
        "             ],\n",
        "             ...\n",
        "         }\n",
        "\n",
        "      2) List of entries with explicit detection lists:\n",
        "\n",
        "         [\n",
        "             {\n",
        "                 \"image_path\": \"<path>\",\n",
        "                 \"detections\": [\n",
        "                     {\"xyxy\": [...], \"conf\": float, \"cls\": int},\n",
        "                     ...\n",
        "                 ]\n",
        "             },\n",
        "             ...\n",
        "         ]\n",
        "\n",
        "         or with the key \"boxes\" instead of \"detections\".\n",
        "\n",
        "    Ground truth boxes and prediction boxes are rescaled to a fixed reference\n",
        "    size given by ``target_size`` before running COCOeval. Tag information is\n",
        "    loaded from a separate COCO style JSON file that stores per image\n",
        "    ``extra.user_tags`` or ``extra.tags`` lists.\n",
        "\n",
        "    Args:\n",
        "        coco_split_dir (str or Path):\n",
        "            Directory containing the COCO split with ``_annotations.coco.json``\n",
        "            and the corresponding images.\n",
        "        predictions (dict or list):\n",
        "            Detector predictions in one of the formats described above.\n",
        "        class_map (dict, optional):\n",
        "            Mapping from YOLO class indices to COCO category ids. If None, the\n",
        "            function tries to infer it from ``yolo_names`` or by searching for\n",
        "            a category whose name contains \"sticker\". If the split only has a\n",
        "            single category, that category is used by default.\n",
        "        yolo_names (list of str, optional):\n",
        "            YOLO class names in index order. Used to construct ``class_map``\n",
        "            automatically when possible.\n",
        "        save_dir (str or Path, optional):\n",
        "            Directory where the COCO detection JSON and optional visualizations\n",
        "            will be written. Created if it does not exist.\n",
        "        target_size (tuple[int, int], optional):\n",
        "            Target image size as ``(width, height)`` used to rescale both\n",
        "            ground truth and predictions before COCOeval.\n",
        "        tags_of_interest (iterable of str, optional):\n",
        "            Tag names that define the tag specific subsets, for example\n",
        "            ``(\"left\", \"right\", \"far\", \"near\")``. Matching is done on the\n",
        "            lowercased tag strings.\n",
        "        tags_coco_json (str or Path, optional):\n",
        "            Path to a COCO style JSON file that contains per image\n",
        "            ``extra.user_tags`` or ``extra.tags`` fields. These tags are used\n",
        "            to assign each image to the tag subsets. If None, only the main\n",
        "            COCO split is used and tag based subsets will be empty.\n",
        "        visualize_img_indices (list[int], optional):\n",
        "            If given, attempt to visualize only images whose file names start\n",
        "            with any of the integer prefixes in this list (for example\n",
        "            ``[149, 164, 194]``). Indices are matched to file names by prefix.\n",
        "            If None, the function selects up to ``vis_limit`` images that have\n",
        "            at least one true positive, false positive, or false negative.\n",
        "        save_visuals (bool, optional):\n",
        "            If True, save visualization figures as PNG files. If False, show\n",
        "            them interactively with ``plt.show()``.\n",
        "        visual_save_dir (str or Path, optional):\n",
        "            Target directory for saved visualizations. If None and\n",
        "            ``save_visuals`` is True, the directory\n",
        "            ``save_dir / \"visualizations\"`` is used.\n",
        "        visual_conf_thresh (float, optional):\n",
        "            Minimum detection confidence required for a prediction to appear in\n",
        "            the visualization panes. Has no effect on COCO metrics, only on\n",
        "            what is drawn.\n",
        "        visual_box_alpha (float, optional):\n",
        "            Alpha value for the bounding box outlines in the visualizations.\n",
        "            Used for all true positive, false positive, and false negative\n",
        "            rectangles.\n",
        "        visual_text_alpha (float, optional):\n",
        "            Alpha value for the text background in the label boxes drawn on\n",
        "            top of each rectangle.\n",
        "        vis_limit (int, optional):\n",
        "            Maximum number of images to visualize when\n",
        "            ``visualize_img_indices`` is None.\n",
        "        side_recall_mode (str, optional):\n",
        "            Strategy used for per tag side recall at IoU 0.5:\n",
        "\n",
        "              * \"manual\" - Uses a simple greedy matching between ground truth\n",
        "                and detections based on IoU, and counts true positives and\n",
        "                false negatives directly.\n",
        "              * \"coco\"   - Builds a side specific COCO subset and uses\n",
        "                COCOeval internals at IoU 0.5 to derive recall.\n",
        "\n",
        "        recall_conf_thresh (float or None, optional):\n",
        "            Confidence threshold used in the \"manual\" side recall mode to\n",
        "            filter detections before matching. If None, all detections are\n",
        "            considered.\n",
        "\n",
        "    Returns:\n",
        "        dict:\n",
        "            A nested result dictionary with keys:\n",
        "\n",
        "            ``\"overall\"``:\n",
        "                Summary for the full split with keys:\n",
        "\n",
        "                * ``\"stats\"``: dict of COCO metrics, including\n",
        "                  ``\"AP@[.5:.95]\"``, ``\"AP@0.50\"``, ``\"AP@0.75\"``, and AR\n",
        "                  values.\n",
        "                * ``\"n_images\"``: number of images in this subset.\n",
        "                * ``\"n_gt\"``: number of ground truth instances for the\n",
        "                  evaluated classes.\n",
        "                * ``\"n_dt\"``: number of detections for the evaluated classes.\n",
        "\n",
        "            One entry per tag in ``tags_of_interest`` (lowercased):\n",
        "                Same structure as ``\"overall\"``, but computed only on images\n",
        "                that contain that tag.\n",
        "\n",
        "            ``\"left_side\"`` and ``\"right_side\"``:\n",
        "                Side aware COCO metrics restricted to the appropriate half of\n",
        "                the windshield, if a windshield category can be identified.\n",
        "                Each entry has the same structure as ``\"overall\"``. If no\n",
        "                windshield category is found, these entries contain zero\n",
        "                counts and ``stats=None``.\n",
        "\n",
        "            ``\"per_tag_side_recall\"``:\n",
        "                Nested dict indexed first by tag (lowercased) and then by side:\n",
        "\n",
        "                * ``results[\"per_tag_side_recall\"][tag][\"left\"]``:\n",
        "                    Manual or COCO based recall at IoU 0.5 for the left half.\n",
        "                * ``results[\"per_tag_side_recall\"][tag][\"right\"]``:\n",
        "                    Same for the right half.\n",
        "                * ``results[\"per_tag_side_recall\"][tag][\"overall\"]``:\n",
        "                    Combined recall across both halves.\n",
        "\n",
        "                Each side dictionary contains:\n",
        "\n",
        "                  * ``\"recall@0.5\"``: recall value at IoU 0.5.\n",
        "                  * ``\"TP\"``: number of true positives.\n",
        "                  * ``\"FN\"``: number of false negatives.\n",
        "                  * ``\"n_images\"``: number of images that contributed ground\n",
        "                    truth or detections for that tag and side.\n",
        "                  * ``\"n_gt\"``: total number of ground truth boxes considered.\n",
        "\n",
        "            ``\"AP50_overall\"``:\n",
        "                Convenience shortcut for\n",
        "                ``results[\"overall\"][\"stats\"][\"AP@0.50\"]``.\n",
        "\n",
        "    Notes:\n",
        "        * The function modifies the COCO ground truth annotations in place by\n",
        "          rescaling bounding boxes and updating image sizes to match\n",
        "          ``target_size``.\n",
        "        * Side aware computations rely on a \"windshield\" category being\n",
        "          present in the COCO categories. If it is missing, side based AP and\n",
        "          side based recall are skipped.\n",
        "        * Visualization requires matplotlib and PIL. If these imports fail,\n",
        "          evaluation still runs but visualizations are skipped.\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------- utils --------------\n",
        "    def _ensure_dir(p: Path):\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "        return p\n",
        "\n",
        "    def _largest_bbox(anns_list):\n",
        "        if not anns_list:\n",
        "            return None\n",
        "        best, best_area = None, -1.0\n",
        "        for a in anns_list:\n",
        "            x, y, w, h = a[\"bbox\"]\n",
        "            area = w * h\n",
        "            if area > best_area:\n",
        "                best_area, best = area, a\n",
        "        return best\n",
        "\n",
        "    def _resolve_image_path(split_dir: Path, file_name: str) -> Optional[Path]:\n",
        "        p = Path(file_name)\n",
        "        cand = []\n",
        "        if p.is_absolute():\n",
        "            cand.append(p)\n",
        "        cand.append(split_dir / p)\n",
        "        cand.append(split_dir / \"images\" / p)\n",
        "        cand.append(split_dir / \"images\" / p.name)\n",
        "        if \"images\" in p.parts:\n",
        "            cand.append(split_dir / \"images\" / p.name)\n",
        "        for c in cand:\n",
        "            if c.exists():\n",
        "                return c\n",
        "        return None\n",
        "\n",
        "    # IoU helpers for manual recall\n",
        "    def _xywh_to_xyxy(box):\n",
        "        x, y, w, h = box\n",
        "        return (x, y, x + w, y + h)\n",
        "\n",
        "    def _iou(a_xywh, b_xywh):\n",
        "        ax1, ay1, ax2, ay2 = _xywh_to_xyxy(a_xywh)\n",
        "        bx1, by1, bx2, by2 = _xywh_to_xyxy(b_xywh)\n",
        "        ix1, iy1 = max(ax1, bx1), max(ay1, by1)\n",
        "        ix2, iy2 = min(ax2, bx2), min(ay2, by2)\n",
        "        iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)\n",
        "        inter = iw * ih\n",
        "        if inter <= 0.0:\n",
        "            return 0.0\n",
        "        a_area = (ax2 - ax1) * (ay2 - ay1)\n",
        "        b_area = (bx2 - bx1) * (by2 - by1)\n",
        "        denom = a_area + b_area - inter\n",
        "        return inter / denom if denom > 0 else 0.0\n",
        "\n",
        "    def _greedy_match_tp(gt_boxes, dt_boxes, iou_thr=0.5):\n",
        "        # dt_boxes is list of (bbox, score)\n",
        "        if not gt_boxes:\n",
        "            return 0, 0\n",
        "        dt_sorted = sorted(dt_boxes, key=lambda z: float(z[1]), reverse=True)\n",
        "        gt_used = [False] * len(gt_boxes)\n",
        "        tp = 0\n",
        "        for db, _sc in dt_sorted:\n",
        "            best_iou, best_idx = 0.0, -1\n",
        "            for i, gb in enumerate(gt_boxes):\n",
        "                if gt_used[i]:\n",
        "                    continue\n",
        "                iou = _iou(gb, db)\n",
        "                if iou > best_iou:\n",
        "                    best_iou, best_idx = iou, i\n",
        "            if best_iou >= iou_thr and best_idx >= 0:\n",
        "                gt_used[best_idx] = True\n",
        "                tp += 1\n",
        "        fn = len(gt_boxes) - tp\n",
        "        return tp, fn\n",
        "\n",
        "    try:\n",
        "        import cv2  # optional\n",
        "    except Exception:\n",
        "        cv2 = None\n",
        "\n",
        "    # -------------- load GT --------------\n",
        "    coco_split_dir = Path(coco_split_dir)\n",
        "    ann_path = coco_split_dir / \"_annotations.coco.json\"\n",
        "    cocoGt = COCO(str(ann_path))\n",
        "    img_ids = cocoGt.getImgIds()\n",
        "    imgs = cocoGt.loadImgs(img_ids)\n",
        "\n",
        "    basename_to_imgid = {Path(im[\"file_name\"]).name: im[\"id\"] for im in imgs}\n",
        "    cats = cocoGt.loadCats(cocoGt.getCatIds())\n",
        "    name_to_catid = {c[\"name\"]: c[\"id\"] for c in cats}\n",
        "\n",
        "    # meta for sub-COCOs\n",
        "    info_meta = cocoGt.dataset.get(\"info\", {})\n",
        "    licenses_meta = cocoGt.dataset.get(\"licenses\", [])\n",
        "\n",
        "    def _build_coco_subset(images_subset, anns_subset, use_eval_cats=True):\n",
        "        sub = COCO()\n",
        "        sub.dataset = {\n",
        "            \"info\": info_meta,\n",
        "            \"licenses\": licenses_meta,\n",
        "            \"images\": images_subset,\n",
        "            \"annotations\": anns_subset,\n",
        "            \"categories\": ([c for c in cats if c[\"id\"] in eval_cat_ids] if use_eval_cats else cats),\n",
        "        }\n",
        "        sub.createIndex()\n",
        "        return sub\n",
        "\n",
        "    # -------------- class_map --------------\n",
        "    if class_map is None:\n",
        "        if yolo_names:\n",
        "            tmp = {i: name_to_catid[nm] for i, nm in enumerate(yolo_names) if nm in name_to_catid}\n",
        "            if tmp:\n",
        "                class_map = tmp\n",
        "        if class_map is None:\n",
        "            sticker_id = next((c[\"id\"] for c in cats if \"sticker\" in c[\"name\"].lower()), None)\n",
        "            if sticker_id:\n",
        "                class_map = {0: sticker_id}\n",
        "    if class_map is None and len(cats) == 1:\n",
        "        class_map = {0: cats[0][\"id\"]}\n",
        "    if class_map is None:\n",
        "        raise ValueError(\"[evaluate] Could not infer class_map on multi-class COCO.\")\n",
        "    eval_cat_ids = sorted(set(class_map.values()))\n",
        "\n",
        "    def to_cat_id(yolo_cls):\n",
        "        return class_map.get(int(yolo_cls), eval_cat_ids[0])\n",
        "\n",
        "    # windshield cat (optional)\n",
        "    windshield_cat_id = None\n",
        "    if yolo_names and \"windshield\" in yolo_names and \"windshield\" in name_to_catid:\n",
        "        windshield_cat_id = name_to_catid[\"windshield\"]\n",
        "    if windshield_cat_id is None:\n",
        "        for c in cats:\n",
        "            if c[\"name\"].lower() == \"windshield\":\n",
        "                windshield_cat_id = c[\"id\"]\n",
        "                break\n",
        "\n",
        "    # for viz color override\n",
        "    windshield_cat_ids = {c[\"id\"] for c in cats if \"windshield\" in c[\"name\"].lower()}\n",
        "\n",
        "    target_w, target_h = target_size\n",
        "\n",
        "    # -------------- scale GT to target (in-place) --------------\n",
        "    imgid_to_origsz = {im[\"id\"]: (im[\"width\"], im[\"height\"]) for im in imgs}\n",
        "    for im in imgs:\n",
        "        ow, oh = im[\"width\"], im[\"height\"]\n",
        "        sx, sy = target_w / ow, target_h / oh\n",
        "        ann_ids_img = cocoGt.getAnnIds(imgIds=[im[\"id\"]])\n",
        "        for ann in cocoGt.loadAnns(ann_ids_img):\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            ann[\"bbox\"] = [x * sx, y * sy, w * sx, h * sy]\n",
        "        im[\"width\"], im[\"height\"] = target_w, target_h\n",
        "\n",
        "    # -------------- normalize predictions --------------\n",
        "    norm_preds = []\n",
        "    if isinstance(predictions, dict):\n",
        "        for k, v in predictions.items():\n",
        "            norm_preds.append({\"image_path\": k, \"detections\": v})\n",
        "    elif isinstance(predictions, list):\n",
        "        for it in predictions:\n",
        "            dets = it.get(\"detections\", it.get(\"boxes\", []))\n",
        "            ip = it.get(\"orig_path\") or it.get(\"image_path\") or it.get(\"crop_path\")\n",
        "            norm_preds.append({\"image_path\": ip, \"detections\": dets})\n",
        "    else:\n",
        "        raise TypeError(\"predictions must be dict or list\")\n",
        "\n",
        "    # -------------- build detection list (scaled) --------------\n",
        "    dt_list, skipped = [], 0\n",
        "    for it in norm_preds:\n",
        "        ipath = it[\"image_path\"]\n",
        "        if not ipath:\n",
        "            continue\n",
        "        base = Path(ipath).name\n",
        "        img_id = basename_to_imgid.get(base)\n",
        "        if img_id is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        ow, oh = imgid_to_origsz[img_id]\n",
        "        sx, sy = target_w / ow, target_h / oh\n",
        "        for d in it.get(\"detections\", []):\n",
        "            x1, y1, x2, y2 = d[\"xyxy\"]\n",
        "            x1, x2 = x1 * sx, x2 * sx\n",
        "            y1, y2 = y1 * sy, y2 * sy\n",
        "            w, h = max(0.0, x2 - x1), max(0.0, y2 - y1)\n",
        "            if w <= 0 or h <= 0:\n",
        "                continue\n",
        "            dt_list.append({\n",
        "                \"image_id\": int(img_id),\n",
        "                \"category_id\": int(to_cat_id(d.get(\"cls\", 0))),\n",
        "                \"bbox\": [float(x1), float(y1), w, h],\n",
        "                \"score\": float(d.get(\"conf\", 0.0))\n",
        "            })\n",
        "    if skipped:\n",
        "        print(f\"[evaluate] Skipped {skipped} prediction entries not found in COCO GT by basename.\")\n",
        "\n",
        "    for cid in eval_cat_ids:\n",
        "        n_gt = len(cocoGt.getAnnIds(catIds=[cid]))\n",
        "        n_dt = sum(1 for d in dt_list if d[\"category_id\"] == cid)\n",
        "        cname = next((c[\"name\"] for c in cats if c[\"id\"] == cid), str(cid))\n",
        "        print(f\"[evaluate] Category '{cname}' (id={cid}): GT={n_gt}, DT={n_dt}\")\n",
        "\n",
        "    # -------------- COCO results --------------\n",
        "    save_dir = Path(save_dir)\n",
        "    _ensure_dir(save_dir)\n",
        "    dt_json = save_dir / f\"coco_dt_{int(time.time())}.json\"\n",
        "    dt_json.write_text(json.dumps(dt_list))\n",
        "    cocoDt = cocoGt.loadRes(str(dt_json)) if len(dt_list) > 0 else COCO()\n",
        "\n",
        "    metric_names = [\n",
        "        \"AP@[.5:.95]\", \"AP@0.50\", \"AP@0.75\", \"AP_small\", \"AP_medium\", \"AP_large\",\n",
        "        \"AR@1\", \"AR@10\", \"AR@100\", \"AR_small\", \"AR_medium\", \"AR_large\"\n",
        "    ]\n",
        "\n",
        "    def run_eval(subset_img_ids, label=\"overall\", coco_gt=None, coco_dt=None):\n",
        "        if not subset_img_ids:\n",
        "            print(f\"[evaluate:{label}] No images in subset.\")\n",
        "            return {\"stats\": None, \"n_images\": 0, \"n_gt\": 0, \"n_dt\": 0}\n",
        "        _coco_gt = coco_gt if coco_gt is not None else cocoGt\n",
        "        _coco_dt = coco_dt if coco_dt is not None else cocoDt\n",
        "        E = COCOeval(_coco_gt, _coco_dt, iouType=\"bbox\")\n",
        "        E.params.imgIds = list(subset_img_ids)\n",
        "        E.params.catIds = eval_cat_ids\n",
        "        E.evaluate()\n",
        "        E.accumulate()\n",
        "        E.summarize()\n",
        "        n_gt = sum(len(_coco_gt.getAnnIds(imgIds=[iid], catIds=eval_cat_ids)) for iid in subset_img_ids)\n",
        "        n_dt = 0\n",
        "        if hasattr(_coco_dt, \"anns\") and _coco_dt.anns:\n",
        "            for _, ann in _coco_dt.anns.items():\n",
        "                if ann.get(\"image_id\") in subset_img_ids and ann.get(\"category_id\") in eval_cat_ids:\n",
        "                    n_dt += 1\n",
        "        return {\n",
        "            \"stats\": {k: float(E.stats[i]) for i, k in enumerate(metric_names)},\n",
        "            \"n_images\": len(subset_img_ids),\n",
        "            \"n_gt\": int(n_gt),\n",
        "            \"n_dt\": int(n_dt),\n",
        "        }\n",
        "\n",
        "    # -------------- tags ingestion --------------\n",
        "    tags_map = {}\n",
        "    if tags_coco_json:\n",
        "        tdata = json.loads(Path(tags_coco_json).read_text())\n",
        "        for im in tdata.get(\"images\", []):\n",
        "            keys = set()\n",
        "            fn = im.get(\"file_name\") or \"\"\n",
        "            en = (im.get(\"extra\") or {}).get(\"name\") or \"\"\n",
        "            for s in (fn, Path(fn).name, Path(fn).stem, en, Path(en).name, Path(en).stem):\n",
        "                if s:\n",
        "                    keys.add(str(s))\n",
        "            raw = (im.get(\"extra\") or {}).get(\"user_tags\") or (im.get(\"extra\") or {}).get(\"tags\") or []\n",
        "            if not isinstance(raw, list):\n",
        "                raw = [raw]\n",
        "            tags = [t.strip().lower() for t in raw if isinstance(t, str) and t.strip()]\n",
        "            for k in keys:\n",
        "                tags_map[k] = tags\n",
        "\n",
        "    imgid_to_tags = {}\n",
        "    for im in imgs:\n",
        "        keys = [\n",
        "            im.get(\"file_name\") or \"\",\n",
        "            Path(im.get(\"file_name\") or \"\").name,\n",
        "            Path(im.get(\"file_name\") or \"\").stem,\n",
        "        ]\n",
        "        tags = []\n",
        "        for k in keys:\n",
        "            if k in tags_map:\n",
        "                tags = tags_map[k]\n",
        "                break\n",
        "        imgid_to_tags[im[\"id\"]] = tags\n",
        "\n",
        "    # -------------- overall + per-tag AP and AR --------------\n",
        "    results = {}\n",
        "    results[\"overall\"] = run_eval(img_ids, \"overall\")\n",
        "    tags_lower = [t.lower() for t in tags_of_interest]\n",
        "    for t in tags_lower:\n",
        "        subset = [iid for iid, tl in imgid_to_tags.items() if t in tl]\n",
        "        print(f\"[evaluate] Tag '{t}': {len(subset)} images.\")\n",
        "        results[t] = run_eval(subset, t)\n",
        "    results[\"AP50_overall\"] = results[\"overall\"][\"stats\"][\"AP@0.50\"] if results[\"overall\"][\"stats\"] else None\n",
        "\n",
        "    # -------------- side-aware AP for left and right images --------------\n",
        "    if windshield_cat_id is not None:\n",
        "        imgid_to_ws = {}\n",
        "        for iid in img_ids:\n",
        "            ws_ids = cocoGt.getAnnIds(imgIds=[iid], catIds=[windshield_cat_id])\n",
        "            if not ws_ids:\n",
        "                continue\n",
        "            best = _largest_bbox(cocoGt.loadAnns(ws_ids))\n",
        "            if not best:\n",
        "                continue\n",
        "            bx, by, bw, bh = best[\"bbox\"]\n",
        "            xmid = bx + bw * 0.5\n",
        "            imgid_to_ws[iid] = {\"bbox\": [bx, by, bw, bh], \"xmid\": xmid}\n",
        "\n",
        "        left_tag_imgs = {iid for iid, tags in imgid_to_tags.items() if \"left\" in tags}\n",
        "        right_tag_imgs = {iid for iid, tags in imgid_to_tags.items() if \"right\" in tags}\n",
        "        left_img_ids_ws = sorted(i for i in left_tag_imgs if i in imgid_to_ws)\n",
        "        right_img_ids_ws = sorted(i for i in right_tag_imgs if i in imgid_to_ws)\n",
        "\n",
        "        def _collect_side_gt(img_id_list, side: str):\n",
        "            out = []\n",
        "            for iid in img_id_list:\n",
        "                meta = imgid_to_ws[iid]\n",
        "                xmid = meta[\"xmid\"]\n",
        "                wx, wy, ww, wh = meta[\"bbox\"]\n",
        "                for a in cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=[iid], catIds=eval_cat_ids)):\n",
        "                    x, y, w, h = a[\"bbox\"]\n",
        "                    cx = x + w * 0.5\n",
        "                    if not (wx <= cx <= wx + ww):\n",
        "                        continue\n",
        "                    if (side == \"left\" and cx <= xmid) or (side == \"right\" and cx > xmid):\n",
        "                        out.append(a)\n",
        "            return out\n",
        "\n",
        "        def _collect_side_dt(img_id_list, side: str):\n",
        "            out = []\n",
        "            for iid in img_id_list:\n",
        "                meta = imgid_to_ws.get(iid)\n",
        "                if not meta:\n",
        "                    continue\n",
        "                xmid = meta[\"xmid\"]\n",
        "                wx, wy, ww, wh = meta[\"bbox\"]\n",
        "                for d in dt_list:\n",
        "                    if d[\"image_id\"] != iid:\n",
        "                        continue\n",
        "                    if d[\"category_id\"] not in eval_cat_ids:\n",
        "                        continue\n",
        "                    x, y, w, h = d[\"bbox\"]\n",
        "                    cx = x + w * 0.5\n",
        "                    if not (wx <= cx <= wx + ww):\n",
        "                        continue\n",
        "                    if (side == \"left\" and cx <= xmid) or (side == \"right\" and cx > xmid):\n",
        "                        out.append(d)\n",
        "            return out\n",
        "\n",
        "        left_gt_anns = _collect_side_gt(left_img_ids_ws, \"left\")\n",
        "        left_imgs = [im for im in imgs if im[\"id\"] in left_img_ids_ws]\n",
        "        coco_left = _build_coco_subset(left_imgs, left_gt_anns)\n",
        "        left_dt = _collect_side_dt(left_img_ids_ws, \"left\")\n",
        "        cocoDt_left = coco_left.loadRes(left_dt) if left_dt else COCO()\n",
        "        results[\"left_side\"] = run_eval(set(left_img_ids_ws), \"left_side\", coco_gt=coco_left, coco_dt=cocoDt_left)\n",
        "\n",
        "        right_gt_anns = _collect_side_gt(right_img_ids_ws, \"right\")\n",
        "        right_imgs = [im for im in imgs if im[\"id\"] in right_img_ids_ws]\n",
        "        coco_right = _build_coco_subset(right_imgs, right_gt_anns)\n",
        "        right_dt = _collect_side_dt(right_img_ids_ws, \"right\")\n",
        "        cocoDt_right = coco_right.loadRes(right_dt) if right_dt else COCO()\n",
        "        results[\"right_side\"] = run_eval(set(right_img_ids_ws), \"right_side\", coco_gt=coco_right, coco_dt=cocoDt_right)\n",
        "    else:\n",
        "        print(\"[evaluate][side-aware] No 'windshield' category; skipping left and right side AP.\")\n",
        "        results[\"left_side\"] = {\"stats\": None, \"n_images\": 0, \"n_gt\": 0, \"n_dt\": 0}\n",
        "        results[\"right_side\"] = {\"stats\": None, \"n_images\": 0, \"n_gt\": 0, \"n_dt\": 0}\n",
        "\n",
        "    # -------------- per-tag side-wise recall@0.5 --------------\n",
        "    results[\"per_tag_side_recall\"] = {}\n",
        "    if windshield_cat_id is not None:\n",
        "        # build once\n",
        "        imgid_to_ws = {}\n",
        "        for iid in img_ids:\n",
        "            ws_ids = cocoGt.getAnnIds(imgIds=[iid], catIds=[windshield_cat_id])\n",
        "            if not ws_ids:\n",
        "                continue\n",
        "            best = _largest_bbox(cocoGt.loadAnns(ws_ids))\n",
        "            if not best:\n",
        "                continue\n",
        "            bx, by, bw, bh = best[\"bbox\"]\n",
        "            xmid = bx + bw * 0.5\n",
        "            imgid_to_ws[iid] = {\"bbox\": [bx, by, bw, bh], \"xmid\": xmid}\n",
        "\n",
        "        # index GT and DT per image\n",
        "        gt_by_img = {\n",
        "            iid: [a for a in cocoGt.loadAnns(cocoGt.getAnnIds(imgIds=[iid], catIds=eval_cat_ids))]\n",
        "            for iid in img_ids\n",
        "        }\n",
        "        dt_by_img = defaultdict(list)\n",
        "        for d in dt_list:\n",
        "            if d[\"category_id\"] in eval_cat_ids:\n",
        "                dt_by_img[d[\"image_id\"]].append(d)\n",
        "\n",
        "        def _side_boxes_from_anns(iid, side, anns, ws_meta):\n",
        "            out = []\n",
        "            xmid = ws_meta[\"xmid\"]\n",
        "            wx, wy, ww, wh = ws_meta[\"bbox\"]\n",
        "            for a in anns:\n",
        "                x, y, w, h = a[\"bbox\"]\n",
        "                cx = x + w * 0.5\n",
        "                if not (wx <= cx <= wx + ww):\n",
        "                    continue\n",
        "                if (side == \"left\" and cx <= xmid) or (side == \"right\" and cx > xmid):\n",
        "                    out.append(a)\n",
        "            return out\n",
        "\n",
        "        def _collect_gt_dt_for_tag_side_manual(tag_str, side):\n",
        "            all_gt_boxes = []\n",
        "            all_dt_boxes = []\n",
        "            contributing_img_ids = set()\n",
        "            for iid, tags in imgid_to_tags.items():\n",
        "                if tag_str not in tags:\n",
        "                    continue\n",
        "                if iid not in imgid_to_ws:\n",
        "                    continue\n",
        "                ws_meta = imgid_to_ws[iid]\n",
        "                # GT side\n",
        "                gt_side_anns = _side_boxes_from_anns(iid, side, gt_by_img[iid], ws_meta)\n",
        "                if gt_side_anns:\n",
        "                    contributing_img_ids.add(iid)\n",
        "                all_gt_boxes.extend([g[\"bbox\"] for g in gt_side_anns])\n",
        "                # DT side\n",
        "                dts = dt_by_img.get(iid, [])\n",
        "                if recall_conf_thresh is None:\n",
        "                    keep = dts\n",
        "                else:\n",
        "                    keep = [d for d in dts if float(d.get(\"score\", 0.0)) >= float(recall_conf_thresh)]\n",
        "                dt_side = _side_boxes_from_anns(iid, side, keep, ws_meta)\n",
        "                if dt_side:\n",
        "                    contributing_img_ids.add(iid)\n",
        "                all_dt_boxes.extend([(d[\"bbox\"], float(d.get(\"score\", 0.0))) for d in dt_side])\n",
        "            return all_gt_boxes, all_dt_boxes, contributing_img_ids\n",
        "\n",
        "        def _collect_gt_dt_for_tag_side_coco(tag_str, side):\n",
        "            imgs_sub = []\n",
        "            gt_anns = []\n",
        "            dt_anns = []\n",
        "            contributing_img_ids = set()\n",
        "            for iid, tags in imgid_to_tags.items():\n",
        "                if tag_str not in tags:\n",
        "                    continue\n",
        "                if iid not in imgid_to_ws:\n",
        "                    continue\n",
        "                ws_meta = imgid_to_ws[iid]\n",
        "                gt_side_anns = _side_boxes_from_anns(iid, side, gt_by_img[iid], ws_meta)\n",
        "                dts = dt_by_img.get(iid, [])\n",
        "                dt_side = _side_boxes_from_anns(iid, side, dts, ws_meta)\n",
        "                if gt_side_anns or dt_side:\n",
        "                    imgs_sub.append(next(i for i in imgs if i[\"id\"] == iid))\n",
        "                    contributing_img_ids.add(iid)\n",
        "                gt_anns.extend(gt_side_anns)\n",
        "                dt_anns.extend(dt_side)\n",
        "            coco_sub = _build_coco_subset(imgs_sub, gt_anns)\n",
        "            cocoDt_sub = coco_sub.loadRes(dt_anns) if dt_anns else COCO()\n",
        "            return coco_sub, cocoDt_sub, [im[\"id\"] for im in imgs_sub], contributing_img_ids\n",
        "\n",
        "        def _recall_at_05_coco(coco_sub, cocoDt_sub, img_ids_sub):\n",
        "            if not img_ids_sub:\n",
        "                return 0.0, 0, 0\n",
        "            E = COCOeval(coco_sub, cocoDt_sub, iouType=\"bbox\")\n",
        "            E.params.imgIds = list(img_ids_sub)\n",
        "            E.params.catIds = eval_cat_ids\n",
        "            E.evaluate()\n",
        "            E.accumulate()\n",
        "            iou_thrs = E.params.iouThrs\n",
        "            t_idx = int(np.argmin(np.abs(iou_thrs - 0.5)))\n",
        "            total_gt, matched_gt = 0, 0\n",
        "            for ei in (E.evalImgs or []):\n",
        "                if ei is None or ei.get(\"category_id\") not in eval_cat_ids:\n",
        "                    continue\n",
        "                gtIds = ei.get(\"gtIds\", [])\n",
        "                gtMatches = ei.get(\"gtMatches\")\n",
        "                gtIgnore = ei.get(\"gtIgnore\", np.zeros(len(gtIds), dtype=bool))\n",
        "                if gtMatches is None:\n",
        "                    continue\n",
        "                gtMatches = np.array(gtMatches)\n",
        "                gtIgnore = np.array(gtIgnore)\n",
        "                if gtMatches.shape[1] != len(gtIds):\n",
        "                    continue\n",
        "                rowg = gtMatches[t_idx]\n",
        "                for k, gid in enumerate(gtIds):\n",
        "                    if gtIgnore[k]:\n",
        "                        continue\n",
        "                    total_gt += 1\n",
        "                    if rowg[k] > 0:\n",
        "                        matched_gt += 1\n",
        "            recall = float(matched_gt) / float(total_gt) if total_gt > 0 else 0.0\n",
        "            return recall, matched_gt, total_gt\n",
        "\n",
        "        for tag in tags_lower:\n",
        "            results[\"per_tag_side_recall\"].setdefault(tag, {})\n",
        "\n",
        "            if side_recall_mode.lower() == \"manual\":\n",
        "                # left\n",
        "                gt_l, dt_l, ids_l = _collect_gt_dt_for_tag_side_manual(tag, \"left\")\n",
        "                tp_l, fn_l = _greedy_match_tp(gt_l, dt_l, iou_thr=0.5)\n",
        "                tot_l = tp_l + fn_l\n",
        "                r_l = (tp_l / tot_l) if tot_l > 0 else 0.0\n",
        "                # right\n",
        "                gt_r, dt_r, ids_r = _collect_gt_dt_for_tag_side_manual(tag, \"right\")\n",
        "                tp_r, fn_r = _greedy_match_tp(gt_r, dt_r, iou_thr=0.5)\n",
        "                tot_r = tp_r + fn_r\n",
        "                r_r = (tp_r / tot_r) if tot_r > 0 else 0.0\n",
        "            else:  # \"coco\"\n",
        "                coco_l, cocoDt_l, img_ids_l, ids_l = _collect_gt_dt_for_tag_side_coco(tag, \"left\")\n",
        "                r_l, tp_l, tot_l = _recall_at_05_coco(coco_l, cocoDt_l, img_ids_l)\n",
        "                coco_r, cocoDt_r, img_ids_r, ids_r = _collect_gt_dt_for_tag_side_coco(tag, \"right\")\n",
        "                r_r, tp_r, tot_r = _recall_at_05_coco(coco_r, cocoDt_r, img_ids_r)\n",
        "\n",
        "            n_img_tag_ws = len(ids_l.union(ids_r))\n",
        "\n",
        "            # store left and right\n",
        "            results[\"per_tag_side_recall\"][tag][\"left\"] = {\n",
        "                \"recall@0.5\": r_l,\n",
        "                \"TP\": tp_l,\n",
        "                \"FN\": tot_l - tp_l if side_recall_mode == \"coco\" else fn_l,\n",
        "                \"n_images\": len(ids_l),\n",
        "                \"n_gt\": tot_l,\n",
        "            }\n",
        "            results[\"per_tag_side_recall\"][tag][\"right\"] = {\n",
        "                \"recall@0.5\": r_r,\n",
        "                \"TP\": tp_r,\n",
        "                \"FN\": tot_r - tp_r if side_recall_mode == \"coco\" else fn_r,\n",
        "                \"n_images\": len(ids_r),\n",
        "                \"n_gt\": tot_r,\n",
        "            }\n",
        "\n",
        "            # overall across left and right\n",
        "            tp_all = tp_l + tp_r\n",
        "            gt_all = tot_l + tot_r\n",
        "            fn_all = gt_all - tp_all\n",
        "            r_all = (tp_all / gt_all) if gt_all > 0 else 0.0\n",
        "            results[\"per_tag_side_recall\"][tag][\"overall\"] = {\n",
        "                \"recall@0.5\": r_all,\n",
        "                \"TP\": tp_all,\n",
        "                \"FN\": fn_all,\n",
        "                \"n_images\": n_img_tag_ws,\n",
        "                \"n_gt\": gt_all,\n",
        "            }\n",
        "    else:\n",
        "        print(\"[evaluate][per-tag side recall] No 'windshield' category; skipping.\")\n",
        "\n",
        "    # -------------- summary print --------------\n",
        "    print(\"\\n=== EVAL SUMMARY ===\")\n",
        "\n",
        "    def _line(lbl, d):\n",
        "        if not d[\"stats\"]:\n",
        "            return f\"{lbl:>12}: n_images={d['n_images']} | n_gt={d['n_gt']} | n_dt={d['n_dt']} (no stats)\"\n",
        "        return (\n",
        "            f\"{lbl:>12}: n_images={d['n_images']} | n_gt={d['n_gt']} | n_dt={d['n_dt']} | \"\n",
        "            f\"AP50={d['stats']['AP@0.50']:.4f} | AP={d['stats']['AP@[.5:.95]']:.4f}\"\n",
        "        )\n",
        "\n",
        "    print(_line(\"overall\", results[\"overall\"]))\n",
        "    for t in tags_lower:\n",
        "        print(_line(t, results[t]))\n",
        "    if \"left_side\" in results:\n",
        "        print(_line(\"left_side\", results[\"left_side\"]))\n",
        "    if \"right_side\" in results:\n",
        "        print(_line(\"right_side\", results[\"right_side\"]))\n",
        "    if \"per_tag_side_recall\" in results:\n",
        "        print(\"\\n=== PER-TAG SIDE RECALL @0.5 ===\")\n",
        "        for tag, sides in results[\"per_tag_side_recall\"].items():\n",
        "            l = sides.get(\"left\", {\"recall@0.5\": 0.0, \"n_gt\": 0, \"n_images\": 0, \"TP\": 0})\n",
        "            r = sides.get(\"right\", {\"recall@0.5\": 0.0, \"n_gt\": 0, \"n_images\": 0, \"TP\": 0})\n",
        "            o = sides.get(\"overall\", {\"recall@0.5\": 0.0, \"n_gt\": 0, \"n_images\": 0, \"TP\": 0})\n",
        "            print(\n",
        "                f\"{tag:>8} | left:  R={l['recall@0.5']:.4f} (TP={l['TP']}/GT={l['n_gt']}, imgs={l['n_images']})\"\n",
        "                f\"   right: R={r['recall@0.5']:.4f} (TP={r['TP']}/GT={r['n_gt']}, imgs={r['n_images']})\"\n",
        "                f\"   overall: R={o['recall@0.5']:.4f} (TP={o['TP']}/GT={o['n_gt']}, imgs={o['n_images']})\"\n",
        "            )\n",
        "\n",
        "    # -------------- visualization (matplotlib) --------------\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import matplotlib.patches as patches\n",
        "        from PIL import Image\n",
        "\n",
        "        Eall = COCOeval(cocoGt, cocoDt, iouType=\"bbox\")\n",
        "        Eall.params.imgIds = list(img_ids)\n",
        "        Eall.params.catIds = eval_cat_ids\n",
        "        Eall.evaluate()\n",
        "        Eall.accumulate()\n",
        "\n",
        "        iou_thrs = Eall.params.iouThrs\n",
        "        t_idx = int(np.argmin(np.abs(iou_thrs - 0.5)))\n",
        "\n",
        "        tp_per_img = defaultdict(list)  # (bbox, cat_id, score)\n",
        "        fp_per_img = defaultdict(list)  # (bbox, cat_id, score)\n",
        "        fn_per_img = defaultdict(list)  # (bbox, cat_id)\n",
        "\n",
        "        dt_anns = {\n",
        "            a[\"id\"]: a\n",
        "            for a in (\n",
        "                cocoDt.loadAnns(cocoDt.getAnnIds())\n",
        "                if hasattr(cocoDt, \"dataset\") and cocoDt.dataset\n",
        "                else []\n",
        "            )\n",
        "        }\n",
        "        gt_anns = {a[\"id\"]: a for a in cocoGt.loadAnns(cocoGt.getAnnIds())}\n",
        "\n",
        "        for ev in (Eall.evalImgs or []):\n",
        "            if ev is None:\n",
        "                continue\n",
        "            img_id = ev[\"image_id\"]\n",
        "            if ev.get(\"category_id\") not in eval_cat_ids:\n",
        "                continue\n",
        "\n",
        "            dtIds = ev.get(\"dtIds\", [])\n",
        "            gtIds = ev.get(\"gtIds\", [])\n",
        "            if len(dtIds) == 0 and len(gtIds) == 0:\n",
        "                continue\n",
        "\n",
        "            dtMatches = np.array(ev.get(\"dtMatches\", []))\n",
        "            gtMatches = np.array(ev.get(\"gtMatches\", []))\n",
        "            dtIgnore = np.array(ev.get(\"dtIgnore\", np.zeros(len(dtIds), dtype=bool)))\n",
        "            gtIgnore = np.array(ev.get(\"gtIgnore\", np.zeros(len(gtIds), dtype=bool)))\n",
        "\n",
        "            # detections\n",
        "            if dtIds is not None:\n",
        "                is_2d = dtMatches.ndim == 2 and dtMatches.shape[0] == len(iou_thrs)\n",
        "                for j, dt_id in enumerate(dtIds):\n",
        "                    ann = dt_anns.get(dt_id)\n",
        "                    if not ann:\n",
        "                        continue\n",
        "                    score = float(ann.get(\"score\", 0.0))\n",
        "                    if score < visual_conf_thresh:\n",
        "                        continue\n",
        "                    if is_2d:\n",
        "                        matched_gt_id = int(dtMatches[t_idx, j]) if j < dtMatches.shape[1] else 0\n",
        "                        is_ignored = (\n",
        "                            bool(dtIgnore[t_idx, j])\n",
        "                            if dtIgnore.ndim == 2\n",
        "                            else bool(dtIgnore[j])\n",
        "                            if dtIgnore.size > j\n",
        "                            else False\n",
        "                        )\n",
        "                    else:\n",
        "                        matched_gt_id = int(dtMatches[j]) if dtMatches.size > j else 0\n",
        "                        is_ignored = bool(dtIgnore[j]) if dtIgnore.size > j else False\n",
        "                    if is_ignored:\n",
        "                        continue\n",
        "                    bbox = ann[\"bbox\"]\n",
        "                    cat_id = ann[\"category_id\"]\n",
        "                    if matched_gt_id > 0:\n",
        "                        tp_per_img[img_id].append((bbox, cat_id, score))\n",
        "                    else:\n",
        "                        fp_per_img[img_id].append((bbox, cat_id, score))\n",
        "\n",
        "            # ground truths\n",
        "            if gtIds is not None:\n",
        "                is_2d = gtMatches.ndim == 2 and gtMatches.shape[0] == len(iou_thrs)\n",
        "                for j, gt_id in enumerate(gtIds):\n",
        "                    ann = gt_anns.get(gt_id)\n",
        "                    if not ann:\n",
        "                        continue\n",
        "                    if is_2d:\n",
        "                        matched_dt_id = int(gtMatches[t_idx, j]) if j < gtMatches.shape[1] else 0\n",
        "                        is_ignored = (\n",
        "                            bool(gtIgnore[t_idx, j])\n",
        "                            if gtIgnore.ndim == 2\n",
        "                            else bool(gtIgnore[j])\n",
        "                            if gtIgnore.size > j\n",
        "                            else False\n",
        "                        )\n",
        "                    else:\n",
        "                        matched_dt_id = int(gtMatches[j]) if gtMatches.size > j else 0\n",
        "                        is_ignored = bool(gtIgnore[j]) if gtIgnore.size > j else False\n",
        "                    if is_ignored:\n",
        "                        continue\n",
        "                    if matched_dt_id == 0:\n",
        "                        bbox = ann[\"bbox\"]\n",
        "                        cat_id = ann[\"category_id\"]\n",
        "                        fn_per_img[img_id].append((bbox, cat_id))\n",
        "\n",
        "        # choose images\n",
        "        if visualize_img_indices is not None:\n",
        "            candidate_imgs = []\n",
        "            for idx in visualize_img_indices:\n",
        "                idx_str = str(idx)\n",
        "                prefix1 = f\"{idx_str}_jpg\"\n",
        "                prefix2 = f\"{idx_str}_\"\n",
        "                matched_img_id = None\n",
        "                for base, _img_id in basename_to_imgid.items():\n",
        "                    if base.startswith(prefix1) or base.startswith(prefix2):\n",
        "                        matched_img_id = _img_id\n",
        "                        break\n",
        "                if matched_img_id is not None and matched_img_id not in candidate_imgs:\n",
        "                    candidate_imgs.append(matched_img_id)\n",
        "\n",
        "            # keep only ones that have any TP, FP, or FN at the applied thresholds\n",
        "            candidate_imgs = [\n",
        "                img_id\n",
        "                for img_id in candidate_imgs\n",
        "                if tp_per_img[img_id] or fp_per_img[img_id] or fn_per_img[img_id]\n",
        "            ]\n",
        "            if not candidate_imgs:\n",
        "                print(f\"[viz] No images matched visualize_img_indices={visualize_img_indices} after thresholding.\")\n",
        "        else:\n",
        "            # take first N in dataset order that have any TP, FP, or FN\n",
        "            candidate_imgs = []\n",
        "            for img_id in img_ids:\n",
        "                if tp_per_img[img_id] or fp_per_img[img_id] or fn_per_img[img_id]:\n",
        "                    candidate_imgs.append(img_id)\n",
        "                if len(candidate_imgs) == vis_limit:\n",
        "                    break\n",
        "\n",
        "        if not candidate_imgs:\n",
        "            print(\"[viz] No images to visualize.\")\n",
        "        else:\n",
        "            if save_visuals:\n",
        "                vis_dir = Path(visual_save_dir) if visual_save_dir else (save_dir / \"visualizations\")\n",
        "                _ensure_dir(vis_dir)\n",
        "\n",
        "            def get_color(base_color, cat_id):\n",
        "                return \"purple\" if cat_id in windshield_cat_ids else base_color\n",
        "\n",
        "            def draw_label(ax, x, y, text):\n",
        "                ax.text(\n",
        "                    x,\n",
        "                    max(y - 2, 0),\n",
        "                    text,\n",
        "                    fontsize=6,\n",
        "                    color=\"white\",\n",
        "                    alpha=visual_text_alpha,\n",
        "                    ha=\"left\",\n",
        "                    va=\"bottom\",\n",
        "                    bbox=dict(\n",
        "                        facecolor=\"black\",\n",
        "                        edgecolor=\"none\",\n",
        "                        pad=1.0,\n",
        "                        alpha=visual_text_alpha,\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            for img_id in candidate_imgs:\n",
        "                img_info = cocoGt.loadImgs([img_id])[0]\n",
        "                img_path = _resolve_image_path(coco_split_dir, img_info[\"file_name\"])\n",
        "                if img_path is None:\n",
        "                    print(f\"[viz] Missing image file for {img_info['file_name']} under {coco_split_dir}, skipping.\")\n",
        "                    continue\n",
        "                try:\n",
        "                    from PIL import Image\n",
        "                    img = Image.open(img_path).convert(\"RGB\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[viz] Could not load image for visualization: {img_path} ({e})\")\n",
        "                    continue\n",
        "                img_resized = img.resize((target_w, target_h))\n",
        "\n",
        "                fig = plt.figure(figsize=(8, 6))\n",
        "                plt.imshow(img_resized)\n",
        "                ax = plt.gca()\n",
        "                ax.set_title(f\"image_id={img_id}\")\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "                # TP\n",
        "                for (bbox, cat_id, score) in tp_per_img.get(img_id, []):\n",
        "                    x, y, w, h = bbox\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x, y),\n",
        "                        w,\n",
        "                        h,\n",
        "                        linewidth=1,\n",
        "                        edgecolor=get_color(\"green\", cat_id),\n",
        "                        facecolor=\"none\",\n",
        "                        alpha=visual_box_alpha,\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    draw_label(ax, x, y, f\"{score:.2f}\")\n",
        "\n",
        "                # FP\n",
        "                for (bbox, cat_id, score) in fp_per_img.get(img_id, []):\n",
        "                    x, y, w, h = bbox\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x, y),\n",
        "                        w,\n",
        "                        h,\n",
        "                        linewidth=1,\n",
        "                        edgecolor=get_color(\"orange\", cat_id),\n",
        "                        facecolor=\"none\",\n",
        "                        alpha=visual_box_alpha,\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    draw_label(ax, x, y, f\"{score:.2f}\")\n",
        "\n",
        "                # FN\n",
        "                for (bbox, cat_id) in fn_per_img.get(img_id, []):\n",
        "                    x, y, w, h = bbox\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x, y),\n",
        "                        w,\n",
        "                        h,\n",
        "                        linewidth=1,\n",
        "                        edgecolor=get_color(\"red\", cat_id),\n",
        "                        facecolor=\"none\",\n",
        "                        alpha=visual_box_alpha,\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    draw_label(ax, x, y, \"miss\")\n",
        "\n",
        "                if save_visuals:\n",
        "                    out_name = f\"vis_{img_id}.png\"\n",
        "                    out_path = (\n",
        "                        Path(visual_save_dir)\n",
        "                        if visual_save_dir\n",
        "                        else (save_dir / \"visualizations\")\n",
        "                    ) / out_name\n",
        "                    fig.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
        "                    plt.close(fig)\n",
        "                    print(f\"[viz] Wrote {out_path}\")\n",
        "                else:\n",
        "                    plt.show()\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"[viz] Visualization skipped (missing dependency): {e}\")\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ecDHWxBgxa0n",
        "hYtbPHrFSDkK",
        "rSOc2QkBSF3p",
        "K1A-pkuIBrve"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
